# DevBench

A comprehensive framework for generating, evaluating, and comparing code completions across multiple programming languages and AI models on our synthetic, telemetry-driven benchmark.

## Overview

DevBench provides tools for:
- Generating standardized code benchmark test cases across multiple programming languages
- Running code completions through various AI models
- Evaluating model performance using multiple metrics
- Analyzing and comparing results across models and languages
- Measuring code complexity metrics of benchmark cases

The benchmark supports multiple programming languages:
- Python
- JavaScript
- TypeScript
- Java
- C++
- C#

## Repository Structure

- `benchmark/` - Generated benchmark test cases organized by language and category
- `completions/` - Model-generated code completions for the benchmark test cases
- `prompts/` - Prompt templates for generating benchmark test cases
- `completion_evaluations/` - Evaluation scripts for analyzing model performance
- `complexities/` - Scripts for measuring code complexity metrics

## Setup

### Requirements

- Python 3.10+
- Conda environment (recommended)
- API keys for various language models

### Installation

1. Clone this repository
2. Create and activate a conda environment:
   ```bash
   conda create -n devbench python=3.10
   conda activate devbench
   ```
3. Install requirements:
   ```bash
   python -m pip install -r requirements.txt
   ```
4. Install Azure CLI following the [official documentation](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest)
5. Authenticate with Azure:
   ```bash
   az login
   ```
6. Set your subscription:
   ```bash
   az account set --subscription <your-subscription-id>
   ```

### Environment Variables Setup

The code requires API keys and configuration for various language models and services. Create a `.env` file in the root directory with the following variables:

```
# Azure OpenAI and Endpoints
ENDPOINT_URL="your_endpoint_url"
DEPLOYMENT_NAME="your_deployment_name"
O3_DEPLOYMENT_NAME="your_o3_deployment_name"
O3MINI_ENDPOINT="your_o3mini_endpoint"
O3MINI_DEPLOYMENT="your_o3mini_deployment"
AZURE_API_KEY="your_azure_api_key"
COMPLETIONS_API_KEY="your_completions_api_key"

# Various Model API Keys
GPT4O_API_KEY="your_gpt4o_api_key"
GPT4OMINI_API_KEY="your_gpt4omini_api_key"
DEEPSEEK_R1_API_KEY="your_deepseek_r1_api_key"
MINISTRAL_API_KEY="your_ministral_api_key"
O3MINI_API_KEY="your_o3mini_api_key"
CLAUDE_API_KEY="your_claude_api_key"
DEEPSEEK_V3_API_KEY="your_deepseek_v3_api_key"

# AWS and Other Service Credentials (to execute the benchmark)
AWS_ACCESS_KEY_ID="your_aws_access_key"
AWS_SECRET_ACCESS_KEY="your_aws_secret_key"
AWS_REGION="your_aws_region"
S3_BUCKET_NAME="your_s3_bucket"
GITHUB_TOKEN="your_github_token"
OPENAI_API_KEY="your_openai_api_key"
```

Replace the placeholder values with your actual API keys and credentials. You may not need all of these keys depending on which models you're using for evaluations.

### Important Note on Script Adaptation

The scripts `generate_benchmark.py`, `generate_completions.py`, and `llm_judge.py` were used for our specific experimental setup, which primarily uses Azure AI Foundry to access various language models. If you're using different methods to access these models, you'll need to modify these scripts accordingly:

- `generate_benchmark.py`: This script may need modifications in how it authenticates and calls language models to generate benchmark test cases. This script contains an anonymized endpoint (e.g., `[ANONYMIZED-ENDPOINT-1]`) and deployment name (e.g., `[ANONYMIZED-DEPLOYMENT-1]`). You will need to replace these with your own valid endpoint and deployment name you're using.
- `generate_completions.py`: This script contains anonymized endpoints (e.g., `[ANONYMIZED-ENDPOINT-1]`) and deployment names (e.g., `[ANONYMIZED-DEPLOYMENT-1]`). You will need to replace these with your own valid endpoints and deployment names for each model service you're using.
- `llm_judge.py`: This script also uses anonymized endpoints (e.g., `[ANONYMIZED-ENDPOINT-2]`) and deployment names (e.g., `[ANONYMIZED-DEPLOYMENT-3]`) for the o3-mini model used as a judge. You will need to replace these with your own valid endpoints and deployment names.

These scripts should be considered templates that demonstrate the methodology rather than plug-and-play solutions. You'll need to:
1. Replace all anonymized endpoints and deployment names with your actual values
2. Adapt the model access methods, API calls, and authentication to match your specific infrastructure
3. Ensure you have the proper API keys and credentials for each model you intend to use

### ⚠️ Security Warning: Executing LLM-Generated Code

**IMPORTANT**: Both the benchmark and model completions evaluation processes involve executing code generated by large language models. This code is **untrusted** and could potentially:

- Contain security vulnerabilities
- Access sensitive data
- Perform unintended or harmful operations
- Execute malicious logic

**Recommended Safety Measures**:
- Run all code execution in a containerized environment (Docker, etc.)
- Use environments with minimal permissions and no access to sensitive systems
- Do not run evaluations on production systems or with production credentials
- Inspect generated code before execution when possible
- Use sandbox environments with network and file system isolation

By using this benchmark system, you accept responsibility for any risks associated with executing AI-generated code. The authors of this project are not responsible for any damage or data loss that may result.

## Usage Guide

### Generating Benchmark Test Cases

Use `generate_benchmark.py` to create benchmark test cases for a specific language and prompt type:

```bash
# Generate 10 Python API usage benchmark test cases
python generate_benchmark.py --generate --completions 10 --language python --prompt-type api_usage

# Generate 10 JavaScript code purpose understanding test cases
python generate_benchmark.py --generate --completions 10 --language javascript --prompt-type code_purpose_understanding
```

Parameters:
- `--generate`: Flag to generate test cases
- `--completions`: Number of test cases to generate (integer)
- `--language`: Target programming language (`python`, `javascript`, `c_sharp`, `cpp`, `typescript`, `java`)
- `--prompt-type`: Type of benchmark test (`api_usage`, `code_purpose_understanding`, etc.)
- `--output`: Optional custom output file path

### Executing Benchmark Tests

There are two ways to execute tests with `generate_benchmark.py`:

#### 1. Testing against Golden Completions

This mode tests the benchmark cases against their golden (correct) completions:

```bash
# Execute all Python benchmark test cases
python generate_benchmark.py --execute --verbose

# Execute specific categories
python generate_benchmark.py --execute --categories api_usage,code_purpose_understanding --verbose

# Execute a specific test case
python generate_benchmark.py --execute --id <test_id> --verbose
```

Parameters:
- `--execute`: Flag to execute test cases
- `--verbose`: Print detailed information during execution
- `--categories`: Comma-separated list of test categories to execute
- `--id`: Run a specific test case with the given ID
- `--report`: Path to output file for detailed test results

#### 2. Evaluating Model Completions

This mode evaluates the model-generated completions against the benchmark tests:

```bash
# Evaluate all model completions
python generate_benchmark.py --execute --model-eval --verbose

# Evaluate specific categories and models
python generate_benchmark.py --execute --model-eval --categories api_usage --models gpt-4o,claude-3-7-sonnet --verbose

# Output results to JSON file
python generate_benchmark.py --execute --model-eval --verbose --json-output benchmark_results.json
```

Parameters:
- `--execute`: Flag to execute test cases
- `--model-eval`: Flag to evaluate model-generated completions
- `--verbose`: Print detailed information during execution
- `--categories`: Comma-separated list of test categories to evaluate
- `--models`: Comma-separated list of model names to evaluate
- `--json-output`: Path to JSON file for saving detailed results
- `--models-dir`: Directory containing model completions (default: completions/python)
- `--report`: Path to output file for detailed test results

### Generating Model Completions

Use `generate_completions.py` to generate completions for benchmark test cases using different models:

```bash
# Generate completions with default settings
python generate_completions.py

# Generate completions with custom settings
python generate_completions.py --output_dir new_completions --temperature 0.2
```

Parameters:
- `--output_dir`: Output directory for completions (default: completions)
- `--temperature`: Temperature for model generation (default: 0.0)

### Evaluating Model Completions

Use `evaluate_completions.py` to compare generated completions against benchmarks:

```bash
# Evaluate completions with default settings
python evaluate_completions.py

# Custom evaluation with different paths
python evaluate_completions.py --completions ../new_completions --benchmark ../benchmark
```

Parameters:
- `--completions`: Path to the completions directory
- `--benchmark`: Path to the benchmark directory
- `--results`: Path for the output results JSON file
- `--plots`: Directory to save visualization plots
- `--debug`: Enable debug mode to print most dissimilar test cases

### Using LLM Judge for Completion Evaluation

Use `llm_judge.py` to evaluate model completions using a language model (o3-mini) as a judge:

```bash
# Evaluate all model completions
python llm_judge.py

# Evaluate specific models
python llm_judge.py --specific_models gpt-4o, claude-3-7-sonnet

# Filter evaluations to specific languages
python llm_judge.py --language python, javascript

# Generate only a summary from existing evaluations
python llm_judge.py --summary_only --plot --heatmap
```

Parameters:
- `--completions_dir`: Directory containing completion files (default: ../completions)
- `--output_dir`: Directory to save evaluation results (default: llm_judge_results)
- `--limit`: Optional limit on the number of files to process per model
- `--max_evaluations`: Optional limit on the total number of evaluations to run
- `--max_file_evaluations`: Optional limit on the number of evaluations per file
- `--summary_only`: Only generate summary without running evaluations
- `--specific_models`: List of specific models to evaluate
- `--language`: List of specific languages to evaluate
- `--plot`: Generate a comparison plot of model scores with confidence intervals
- `--heatmap`: Generate language-category heatmaps for models

### Comparing Model Performance Across Languages

Use `compare_model_languages.py` to analyze how models perform across different programming languages:

```bash
python compare_model_languages.py
```

The script analyzes models' strengths and weaknesses across different programming languages, identifying language preferences and trends.

### Analyzing Code Complexity

Use `calculate_complexity.py` to analyze complexity metrics of benchmark cases:

```bash
python calculate_complexity.py
```

This script calculates various code complexity metrics including:
- Line count
- Token count
- Cyclomatic complexity
