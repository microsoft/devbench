
================================================================================
Test Case #1
================================================================================

================================================================================
Test Case ID: 1
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import os

def read_file(file_path):
    try:

LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        with open(file_path, 'r') as f:
            return f.read()

SUFFIX CODE:
----------------------------------------

    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

# Test cases
try:
    with open('test.txt', 'w') as f:
        f.write('Hello, world!')
    content = read_file('test.txt')
    assert content == 'Hello, world!'
except Exception as e:
    assert False, f'Exception occurred: {e}'

JUSTIFICATION:
----------------------------------------
This test case demonstrates a common pattern of file handling in Python, which involves opening a file, reading its contents, and handling the potential FileNotFoundError. The completion should recognize the need to read the file content and handle exceptions properly. The scenario is designed to test the LLM's ability to identify and continue a standard file I/O pattern with minimal context.

ASSERTIONS:
----------------------------------------
assert read_file('non_existent.txt') == '', 'Should return empty string if file does not exist'
try:
    with open('example.txt', 'w') as f:
        f.write('Example content')
    content = read_file('example.txt')
    assert content == 'Example content', 'Content should match written content'
except Exception as e:
    assert False, f'Exception occurred: {e}'
finally:
    if os.path.exists('example.txt'):
        os.remove('example.txt')
================================================================================

================================================================================
Test Case #2
================================================================================

================================================================================
Test Case ID: 2
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools
import time

def timer(func):
    """Print the runtime of the decorated function"""
    @functools.wraps(func)
    def wrapper_timer(*args, **kwargs):
        start_time = time.perf_counter()
        value = func(*args, **kwargs)
        end_time = time.perf_counter()
        run_time = end_time - start_time
        print(f"Finished {func.__name__!r} in {run_time:.4f} secs")
        return value
    return wrapper_timer

@timer
def waste_some_time(num_times):
    for _ in range(num_times):
        sum([i**2 for i in range(10000)])

@timer
def do_something():
    print("Doing something important...")


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
do_something()

SUFFIX CODE:
----------------------------------------
do_something()

@timer
def another_function():
    time.sleep(1)
    return "Done"

result = another_function()
assert result == "Done"


JUSTIFICATION:
----------------------------------------
This example tests the LLM's ability to recognize and complete the decorator pattern. The prefix provides a detailed setup of the timer decorator, which measures the execution time of the decorated functions. The suffix demonstrates the usage of the timer decorator on a different function and includes assertions to verify the correctness of another function. The completion must correctly use the established timer decorator pattern with minimal context provided.

ASSERTIONS:
----------------------------------------
waste_some_time(5)

================================================================================

================================================================================
Test Case #3
================================================================================

================================================================================
Test Case ID: 3
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools

def log_execution(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        print(f'Executing {func.__name__}...')
        result = func(*args, **kwargs)
        print(f'{func.__name__} executed.')
        return result
    return wrapper

@log_execution


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
def add(a, b):
    return a + b

SUFFIX CODE:
----------------------------------------


result = add(3, 5)

assert result == 8


JUSTIFICATION:
----------------------------------------
This test case assesses the LLM's ability to recognize and continue a decorator pattern in Python. The prefix sets up a common logging decorator, and the completion should involve applying this decorator to a simple function. The suffix differs from the completion to ensure the test case evaluates understanding of pattern continuation, not just copy-pasting.

ASSERTIONS:
----------------------------------------
assert result == 8

================================================================================

================================================================================
Test Case #4
================================================================================

================================================================================
Test Case ID: 4
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
from functools import wraps
import time

# Decorator to measure the execution time of a function

def timeit(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"Function {func.__name__} took {end_time - start_time:.4f} seconds")
        return result
    return wrapper

@timeit
def slow_function(seconds):
    time.sleep(seconds)
    return f"Slept for {seconds} seconds"

@timeit
def fast_function():
    return "Done quickly"

# Testing the decorators
result_slow = slow_function(2)
result_fast = fast_function()
# Check the results


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
assert result_slow == "Slept for 2 seconds"
assert result_fast == "Done quickly"

SUFFIX CODE:
----------------------------------------
# Clean up resources if any (not needed in this example)
print("All assertions passed.")


JUSTIFICATION:
----------------------------------------
This test case is a good example of decorator usage, specifically for measuring execution time. The pattern is clear and well-established, and the prefix provides comprehensive context for the decorator's implementation. The golden completion tests the decorator's functionality, while the suffix provides different code to avoid duplication. Assertions ensure the decorator works as expected, and the test case covers multiple valid completions.

ASSERTIONS:
----------------------------------------
assert 'Slept for 2 seconds' in result_slow
assert result_fast == 'Done quickly'

================================================================================

================================================================================
Test Case #5
================================================================================

================================================================================
Test Case ID: 5
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools

def trace(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        print(f'TRACE: calling {func.__name__}() with {args}, {kwargs}')
        result = func(*args, **kwargs)
        print(f'TRACE: {func.__name__}() returned {result}')
        return result
    return wrapper

class Calculator:
    def __init__(self):
        self.value = 0

    @trace
    def add(self, amount):
        self.value += amount
        return self.value

    @trace
    def subtract(self, amount):
        self.value -= amount
        return self.value

    # Missing method implementation


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    @trace
    def multiply(self, amount):
        self.value *= amount
        return self.value

SUFFIX CODE:
----------------------------------------

    @trace
    def divide(self, amount):
        if amount == 0:
            print('Cannot divide by zero')
            return
        self.value /= amount
        return self.value

calc = Calculator()
calc.add(10)
calc.subtract(5)
calc.divide(2)
assert calc.value == 2.5
calc.divide(0)  # This should raise an exception


JUSTIFICATION:
----------------------------------------
This test case is a good example for evaluating an LLM's ability to recognize and complete a pattern involving decorator usage in a class. The prefix provides a clear pattern of methods within the Calculator class being decorated with a trace decorator, and the missing implementation of the multiply method follows the same pattern. The suffix provides a different method implementation (divide) to ensure that the completion is distinct. The assertions check for correct functionality and error handling, ensuring that the pattern is correctly followed.

ASSERTIONS:
----------------------------------------
calc = Calculator()
calc.add(10)
assert calc.value == 10
calc.multiply(2)
assert calc.value == 20
calc.subtract(5)
assert calc.value == 15
calc.divide(3)
assert calc.value == 5.0
try:
    calc.divide(0)
except ValueError as e:
    assert str(e) == 'Cannot divide by zero'
================================================================================

================================================================================
Test Case #6
================================================================================

================================================================================
Test Case ID: 6
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import time
import threading

class EventManager:
    def __init__(self):
        self.listeners = []

    def register_listener(self, listener):
        self.listeners.append(listener)

    def trigger_event(self, event_data):
        for listener in self.listeners:
            listener(event_data)

# A sample listener function

def sample_listener(event_data):
    print(f"Event received with data: {event_data}")

# Instantiate the EventManager and register the listener
manager = EventManager()
manager.register_listener(sample_listener)

# Function to simulate event trigger
def simulate_event_trigger(manager):
    time.sleep(1)
    manager.trigger_event("Test Event")

# Start the event simulation in a separate thread
thread = threading.Thread(target=simulate_event_trigger, args=(manager,))
thread.start()


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
thread.join()

SUFFIX CODE:
----------------------------------------
# Another listener function

def another_listener(event_data):
    print(f"Another event received with data: {event_data}")

# Register another listener and trigger another event
manager.register_listener(another_listener)
simulate_event_trigger(manager)

assert 'sample_listener' in [listener.__name__ for listener in manager.listeners]
assert 'another_listener' in [listener.__name__ for listener in manager.listeners]


JUSTIFICATION:
----------------------------------------
This example tests the model's ability to recognize and complete an event handling pattern. The prefix sets up an event manager, registers a listener, and simulates an event trigger in a separate thread. The golden completion, joining the thread, is crucial for ensuring that the event handling completes before proceeding. The suffix registers another listener and triggers another event, checking that both listeners are registered correctly.

ASSERTIONS:
----------------------------------------
assert 'sample_listener' in [listener.__name__ for listener in manager.listeners]
assert 'another_listener' in [listener.__name__ for listener in manager.listeners]

================================================================================

================================================================================
Test Case #7
================================================================================

================================================================================
Test Case ID: 7
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools
import time

# This is a decorator to log the execution time of functions

def log_execution_time(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"{func.__name__} took {end_time - start_time:.4f} seconds")
        return result
    return wrapper

# Example function to demonstrate the decorator

@log_execution_time
def example_function(n):
    """ Function that sleeps for n seconds """
    time.sleep(n)
    return f"Slept for {n} seconds"

# Another function that should use the same decorator
@log_execution_time
def another_example_function():
    """ Function that performs a quick task """


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    return "Quick task completed"

SUFFIX CODE:
----------------------------------------
    return "Quick task completed"

# Assertions to test the functionality of the decorator
result = example_function(2)
assert result == "Slept for 2 seconds"

result = another_example_function()
assert result == "Quick task completed"

# Cleanup code (if any) would go here


JUSTIFICATION:
----------------------------------------
This test case evaluates an LLM's ability to recognize and complete a decorator pattern in Python with minimal context. The prefix establishes the pattern by defining a decorator function and applying it to `example_function`. The golden completion requires the LLM to continue this pattern by correctly implementing the body of `another_example_function` to include a quick task. The suffix validates the pattern adherence with assertions. This scenario tests the LLM's understanding of decorators and their application, ensuring it can recognize and extend patterns based on limited context.

ASSERTIONS:
----------------------------------------
result = example_function(2)
assert result == "Slept for 2 seconds"

result = another_example_function()
assert result == "Quick task completed"

================================================================================

================================================================================
Test Case #8
================================================================================

================================================================================
Test Case ID: 8
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools

class EventEmitter:
    def __init__(self):
        self.listeners = {}

    def on(self, event, listener):
        if event not in self.listeners:
            self.listeners[event] = []
        self.listeners[event].append(listener)

    def emit(self, event, *args, **kwargs):
        if event in self.listeners:
            for listener in self.listeners[event]:
                listener(*args, **kwargs)

emitter = EventEmitter()

# Register a callback for the 'data' event
emitter.on('data', lambda x: print(f'Data received: {x}'))

# Register a callback for the 'error' event


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
emitter.on('error', lambda e: print(f'Error: {e}'))

SUFFIX CODE:
----------------------------------------
# Emit 'error' event
emitter.emit('error', 'Something went wrong')
# Emit 'data' event
emitter.emit('data', 42)



JUSTIFICATION:
----------------------------------------
This test case examines the LLM's ability to recognize and complete a common callback registration pattern in an event handling system. The pattern involves registering callbacks for specific events and then emitting those events. The prefix sets up the EventEmitter class and registers a callback for a 'data' event. The golden completion requires recognizing that another callback should be registered for the 'error' event, following the same pattern. The suffix ensures that different events are emitted to test the callbacks.

ASSERTIONS:
----------------------------------------
emitter.emit('error', 'Something went wrong')
emitter.emit('data', 42)
assert emitter.listeners['data'][0]('Test data') is None
assert emitter.listeners['error'][0]('Test error') is None

# Cleanup
emitter.listeners.clear()
================================================================================

================================================================================
Test Case #9
================================================================================

================================================================================
Test Case ID: 9
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools

def log_execution(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        print(f'Calling {func.__name__} with args: {args} and kwargs: {kwargs}')
        result = func(*args, **kwargs)
        print(f'{func.__name__} returned {result}')
        return result
    return wrapper

@log_execution
def add(a, b):
    return a + b

@log_execution


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
def multiply(a, b):

SUFFIX CODE:
----------------------------------------
def multiply(a, b):
    return a * b

result_add = add(2, 3)
result_multiply = multiply(2, 3)

assert result_add == 5
assert result_multiply == 6


JUSTIFICATION:
----------------------------------------
This example tests the model's ability to recognize and complete a decorator pattern. The prefix establishes a logging decorator and applies it to an `add` function. The completion should define another function, `subtract`, following the same pattern of being decorated with `@log_execution`. This is a good test case because it requires the model to recognize the decorator pattern and apply it consistently with minimal context.

ASSERTIONS:
----------------------------------------
result_subtract = subtract(5, 2)
assert result_subtract == 3
================================================================================

================================================================================
Test Case #10
================================================================================

================================================================================
Test Case ID: 10
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools
import logging

logging.basicConfig(level=logging.DEBUG)

class EventHandler:
    def __init__(self):
        self._callbacks = []

    def register_callback(self, func):
        self._callbacks.append(func)

    def handle_event(self, *args, **kwargs):
        for callback in self._callbacks:
            callback(*args, **kwargs)

class Logger:
    def __init__(self, name):
        self.logger = logging.getLogger(name)

    def log(self, message):
        self.logger.debug(message)

handler = EventHandler()
logger = Logger('testLogger')

# Decorator to log function calls


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
def log_calls(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        logger.log(f"Calling {func.__name__} with args={args}, kwargs={kwargs}")
        return func(*args, **kwargs)
    return wrapper

@log_calls
def process_event(event):
    logger.log(f"Processing event: {event}")


SUFFIX CODE:
----------------------------------------
handler.register_callback(process_event)
handler.register_callback(logger.log)

handler.handle_event('Event occurred!')



JUSTIFICATION:
----------------------------------------
This test case evaluates the LLM's ability to recognize and complete a decorator pattern within the context of callback and event handling. The prefix sets up an event handler and a logger, introducing the need for a decorator to log function calls. The completion should correctly implement a logging decorator and register it as a callback. This tests the LLM's understanding of decorators, logging, and event handling patterns.

ASSERTIONS:
----------------------------------------
assert len(handler._callbacks) == 2
assert handler._callbacks[1].__name__ == 'log'
assert handler._callbacks[1].__self__.logger.name == 'testLogger'

================================================================================

================================================================================
Test Case #11
================================================================================

================================================================================
Test Case ID: 11
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools

class EventEmitter:
    def __init__(self):
        self._events = {}

    def on(self, event, callback=None):
        # When used as a decorator, only event is provided
        if callback is None:
            # Return a decorator function that will receive the actual callback
            def decorator(func):
                if event not in self._events:
                    self._events[event] = []
                self._events[event].append(func)
                return func
            return decorator
        else:
            # Regular method usage
            if event not in self._events:
                self._events[event] = []
            self._events[event].append(callback)

    def emit(self, event, *args, **kwargs):
        if event in self._events:
            for callback in self._events[event]:
                callback(*args, **kwargs)

emitter = EventEmitter()

# Define a decorator to register an event callback


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
@emitter.on('user_logged_in')

SUFFIX CODE:
----------------------------------------
def on_user_logged_in(user):
    print(f"User {user} has logged in.")

emitter.emit('user_logged_in', 'Alice')

# Assertions
assert 'user_logged_in' in emitter._events
assert len(emitter._events['user_logged_in']) == 1
assert callable(emitter._events['user_logged_in'][0])


JUSTIFICATION:
----------------------------------------
This test case is a good example of testing low-context pattern matching capabilities because it involves recognizing a common decorator pattern used to register event callbacks. The prefix provides enough context to understand that a decorator is needed to register an event callback, but the completion itself requires understanding the specific event and the decorator syntax. The suffix demonstrates the event being emitted and includes assertions to verify the correct behavior, making it clear and distinct from the golden completion.

ASSERTIONS:
----------------------------------------
emitter.emit('user_logged_in', 'Alice')
assert 'user_logged_in' in emitter._events
assert len(emitter._events['user_logged_in']) == 1
assert callable(emitter._events['user_logged_in'][0])

================================================================================

================================================================================
Test Case #12
================================================================================

================================================================================
Test Case ID: 12
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools
import io
import sys

class EventSystem:
    def __init__(self):
        self._events = {}

    def on(self, event_name, callback):
        if event_name not in self._events:
            self._events[event_name] = []
        self._events[event_name].append(callback)

    def trigger(self, event_name, *args, **kwargs):
        if event_name in self._events:
            for callback in self._events[event_name]:
                callback(*args, **kwargs)

    def clear_event(self, event_name):
        if event_name in self._events:
            del self._events[event_name]

# Example callback functions
def on_data_received(data):
    print(f"Data received: {data}")

def on_error_occurred(error):
    print(f"Error: {error}")

# Create an event system instance
event_system = EventSystem()

# Register events


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
event_system.on("data_received", on_data_received)
event_system.on("error_occurred", on_error_occurred)

SUFFIX CODE:
----------------------------------------


JUSTIFICATION:
----------------------------------------
This test case evaluates the model's ability to recognize and complete an event handling pattern. The prefix sets up an event system with registration and triggering of events, providing context for how callbacks should be registered and triggered. The golden completion correctly adds event listeners and triggers them, while the suffix tests the triggering and clearing of events. The scenario is realistic and tests the model's ability to continue established patterns in event handling code.

ASSERTIONS:
----------------------------------------
import io
import sys

# Redirect stdout to capture print outputs
captured_output = io.StringIO()
sys.stdout = captured_output

# Trigger events
try:
    event_system.trigger('data', 'Hello, World!')
    event_system.trigger('error', '404 Not Found')
except Exception as e:
    print(f"Exception occurred: {e}")
finally:
    # Reset redirect.
    sys.stdout = sys.__stdout__

# Verify the captured output
output = captured_output.getvalue().strip().split('\n')
assert output[0] == 'Data received: Hello, World!'
assert output[1] == 'Error: 404 Not Found'
assert len(output) == 2

================================================================================

================================================================================
Test Case #13
================================================================================

================================================================================
Test Case ID: 13
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import time
import threading

class EventListener:
    def __init__(self):
        self.events = []

    def on_event(self, callback):
        self.events.append(callback)

    def trigger_event(self, event_data):
        for callback in self.events:
            callback(event_data)

class EventSource:
    def __init__(self):
        self.listener = EventListener()

    def start(self):
        def run():
            for i in range(5):
                time.sleep(1)
                self.listener.trigger_event(f'Event {i}')
        threading.Thread(target=run).start()

source = EventSource()

# Register a callback to handle events
source.listener.on_event(lambda data: print(f'Received: {data}'))

source.start()


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
captured_output = []
source.listener.on_event(lambda data: captured_output.append(data))

SUFFIX CODE:
----------------------------------------
source.listener.on_event(lambda data: print(f'Handled: {data}'))

# Ensuring all events are processed
import time

# Wait for the events to be processed
for _ in range(6):
    time.sleep(1)

assert 'Event 0' in captured_output
assert 'Event 1' in captured_output
assert 'Event 2' in captured_output
assert 'Event 3' in captured_output
assert 'Event 4' in captured_output

# Clean up any resources if necessary
# (None in this example as threading automatically cleans up)


JUSTIFICATION:
----------------------------------------
This test case is a good example of callback and event handling patterns. It sets up an event listener and an event source, then triggers events and verifies that callbacks are executed correctly. The completion involves capturing output using a list, which is a common pattern in handling callbacks. Multiple valid completions may exist, such as different ways to capture and verify the events.

ASSERTIONS:
----------------------------------------
assert 'Event 0' in captured_output
assert 'Event 1' in captured_output
assert 'Event 2' in captured_output
assert 'Event 3' in captured_output
assert 'Event 4' in captured_output

================================================================================

================================================================================
Test Case #14
================================================================================

================================================================================
Test Case ID: 14
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import sqlite3

class Database:
    def __init__(self, db_name):
        self.connection = sqlite3.connect(db_name)
        self.cursor = self.connection.cursor()

    def create_table(self):
        self.cursor.execute("CREATE TABLE IF NOT EXISTS users (id INTEGER PRIMARY KEY, name TEXT)")

    def insert_user(self, name):
        self.cursor.execute("INSERT INTO users (name) VALUES (?)", (name,))
        self.connection.commit()

    def fetch_users(self):
        self.cursor.execute("SELECT * FROM users")
        return self.cursor.fetchall()

    def close(self):
        self.connection.close()

# Create an instance of the Database class


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
db = Database('test.db')
db.create_table()
db.insert_user('John Doe')

SUFFIX CODE:
----------------------------------------

# Fetch users and validate the insert
users = db.fetch_users()
assert len(users) == 1
assert users[0][1] == 'John Doe'



JUSTIFICATION:
----------------------------------------
This test case is a good example of a common usage pattern involving database operations. It includes creating a table, inserting a record, and fetching the record, which are frequent tasks in many applications. The LLM needs to recognize the pattern of creating an instance, performing operations, and ensuring cleanup. This example tests the LLM's ability to follow a standard sequence of database operations and handle minimal context effectively.

ASSERTIONS:
----------------------------------------
assert db.fetch_users() == [(1, 'John Doe')]

# Close the database connection
db.close()

================================================================================

================================================================================
Test Case #15
================================================================================

================================================================================
Test Case ID: 15
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools

class Event:
    def __init__(self):
        self._callbacks = []

    def register(self, callback):
        self._callbacks.append(callback)

    def emit(self, *args, **kwargs):
        for callback in self._callbacks:
            callback(*args, **kwargs)

class Button:
    def __init__(self):
        self.click_event = Event()

    def click(self):
        self.click_event.emit()

button = Button()

# Registering a callback


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
def on_click():
    print("Button clicked!")

button.click_event.register(on_click)

SUFFIX CODE:
----------------------------------------
# Another action on click
@functools.lru_cache(maxsize=None)
def cached_click_action():
    print("Cached click action executed")

button.click_event.register(cached_click_action)

# Simulating button clicks
button.click()
button.click()

assert len(button.click_event._callbacks) == 2, "There should be two callbacks registered"
assert not button.click_event._callbacks[1] is button.click_event._callbacks[0], "Callbacks should be different instances"


JUSTIFICATION:
----------------------------------------
This is a good test case for callback and event handling patterns because it demonstrates how to register and emit events with different kinds of callbacks. The prefix sets up a clear event system and the suffix shows another way to register a callback, testing the LLM's ability to recognize and continue the pattern by registering a simple lambda function as a callback. This tests the LLM's understanding of event-driven programming and callback registration.

ASSERTIONS:
----------------------------------------
button.click()
button.click()

assert len(button.click_event._callbacks) == 2, "There should be two callbacks registered"
assert not button.click_event._callbacks[1] is button.click_event._callbacks[0], "Callbacks should be different instances"

================================================================================

================================================================================
Test Case #16
================================================================================

================================================================================
Test Case ID: 16
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
def read_file(file_path):
    try:
        with open(file_path, 'r') as file:
            content = file.read()
    except FileNotFoundError:
        return None


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    return content

SUFFIX CODE:
----------------------------------------


# Test the function
file_path = 'test_file.txt'
with open(file_path, 'w') as f:
    f.write('Hello, world!')

result = read_file(file_path)
assert result == 'Hello, world!'
assert read_file('non_existent_file.txt') is None


JUSTIFICATION:
----------------------------------------
This test case focuses on a universal pattern of reading a file, handling exceptions, and performing cleanup. It tests the model's ability to complete the function correctly by returning the content read from the file. The scenario includes proper exception handling for a common file operation and ensures cleanup after assertions.

ASSERTIONS:
----------------------------------------
assert result == 'Hello, world!'
assert read_file('non_existent_file.txt') is None
================================================================================

================================================================================
Test Case #17
================================================================================

================================================================================
Test Case ID: 17
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import os

def create_temp_file():
    temp_file = 'temp.txt'
    with open(temp_file, 'w') as f:
        f.write('Temporary data')
    return temp_file

temp_file = create_temp_file()

try:
    # Read the file content

LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    with open(temp_file, 'r') as f:
        content = f.read()
        print(content)

SUFFIX CODE:
----------------------------------------

finally:
    if os.path.exists(temp_file):
        os.remove(temp_file)
        print(f"Cleaned up {temp_file}")

JUSTIFICATION:
----------------------------------------
This example tests the model's ability to recognize the standard pattern of working with temporary files in Python. The completion should demonstrate the ability to open and read from a file in a context where the file is created and cleaned up properly. This pattern is widely used and follows well-established conventions.

ASSERTIONS:
----------------------------------------
assert content == 'Temporary data'
assert os.path.exists(temp_file) == False
================================================================================

================================================================================
Test Case #18
================================================================================

================================================================================
Test Case ID: 18
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import os

def create_and_write_file(file_path, content, auto_delete=True):
    try:
        # Create the file and write content to it


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        with open(file_path, 'w') as file:
            file.write(content)

SUFFIX CODE:
----------------------------------------
    finally:
        # Clean up: delete the file if auto_delete is True
        if auto_delete and os.path.exists(file_path):
            os.remove(file_path)

# Test the function
file_path = 'testfile.txt'
content = 'Hello, World!'
create_and_write_file(file_path, content)


JUSTIFICATION:
----------------------------------------
This is a good test case because it involves a common file operation pattern: creating and writing to a file within a context manager to ensure proper handling of file resources. The context manager pattern is widely used and recognized in Python, making it a universal pattern. Additionally, it tests the LLM's ability to handle resource cleanup correctly with minimal context provided.

ASSERTIONS:
----------------------------------------
assert not os.path.exists(file_path), 'File should be deleted after function execution'
try:
    create_and_write_file(file_path, content, auto_delete=False)
    with open(file_path, 'r') as file:
        assert file.read() == content, 'File content should match the written content'
finally:
    if os.path.exists(file_path):
        os.remove(file_path)

================================================================================

================================================================================
Test Case #19
================================================================================

================================================================================
Test Case ID: 19
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import os
import tempfile

class FileManager:
    def __init__(self, filename):
        self.filename = filename

    def write_data(self, data):


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        with open(self.filename, 'w') as f:
            f.write(data)

SUFFIX CODE:
----------------------------------------

    def read_data(self):
        with open(self.filename, 'r') as f:
            return f.read()

# Create a temporary file
with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
    file_path = tmp_file.name

# Initialize FileManager
manager = FileManager(file_path)

# Write data to file
manager.write_data('Sample Data')

# Read data from file
assert manager.read_data() == 'Sample Data'



JUSTIFICATION:
----------------------------------------
This scenario tests the model's ability to recognize a common pattern in file operations where data is written to a file. The pattern of opening a file, writing data to it, and closing the file is widely used and should be recognizable with minimal context. The completion demonstrates understanding of how to correctly open a file in write mode and write data to it, which is a universal programming pattern.

ASSERTIONS:
----------------------------------------
assert os.path.exists(manager.filename)
manager.write_data('Test Data')
assert manager.read_data() == 'Test Data'

# Cleanup
os.remove(file_path)

================================================================================

================================================================================
Test Case #20
================================================================================

================================================================================
Test Case ID: 20
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import json

def save_to_file(data, filename):
    with open(filename, 'w') as file:


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        json.dump(data, file)

SUFFIX CODE:
----------------------------------------


JUSTIFICATION:
----------------------------------------
This test case examines the ability to recognize a standard file operation pattern in Python with minimal context. It focuses on using the json module to serialize data and write it to a file within a 'with' statement context. The pattern of using 'json.dump' for writing JSON data to a file is common, and the minimal context should be sufficient for recognizing this standard operation. The provided golden completion aligns with best practices for writing JSON data to a file.

ASSERTIONS:
----------------------------------------
data = {'key': 'value'}
filename = 'testfile.json'
save_to_file(data, filename)

with open(filename, 'r') as file:
    content = file.read()
    assert content == '{"key": "value"}'

# Cleanup
import os
os.remove(filename)

================================================================================

================================================================================
Test Case #21
================================================================================

================================================================================
Test Case ID: 21
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import json

def load_json(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)
        return data



LLM COMPLETION gpt-4.5-preview:
----------------------------------------
def save_json(file_path, data):
    with open(file_path, 'w') as file:
        json.dump(data, file)

SUFFIX CODE:
----------------------------------------


# Prepare test data
file_path = 'test.json'
save_json(file_path, {'name': 'example'})
data = load_json(file_path)

# Assertions to verify correctness
assert isinstance(data, dict)
assert 'name' in data
assert data['name'] == 'example'



JUSTIFICATION:
----------------------------------------
This example tests the model's ability to recognize a common file I/O pattern, specifically loading and saving JSON data. The task is to complete the function that saves JSON data to a file, following the pattern established by the load_json function. This pattern is universal and follows well-established conventions. The assertions verify that the data is correctly saved and loaded, ensuring the completion maintains consistency with the prefix and suffix.

ASSERTIONS:
----------------------------------------
assert isinstance(data, dict)
assert 'name' in data
assert data['name'] == 'example'

# Cleanup
import os
os.remove(file_path)
================================================================================

================================================================================
Test Case #22
================================================================================

================================================================================
Test Case ID: 22
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import os
import tempfile

class FileManager:
    def __init__(self):
        self.temp_dir = tempfile.TemporaryDirectory()

    def create_temp_file(self, filename, content):
        file_path = os.path.join(self.temp_dir.name, filename)
        with open(file_path, 'w') as f:
            f.write(content)
        return file_path

    def read_temp_file(self, filename):
        file_path = os.path.join(self.temp_dir.name, filename)
        with open(file_path, 'r') as f:
            return f.read()

    def delete_temp_file(self, filename):
        file_path = os.path.join(self.temp_dir.name, filename)
        if os.path.exists(file_path):


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
            os.remove(file_path)

SUFFIX CODE:
----------------------------------------


    def __del__(self):
        self.temp_dir.cleanup()

# Example usage
fm = FileManager()
file_path = fm.create_temp_file('test.txt', 'Hello, World!')
content = fm.read_temp_file('test.txt')
fm.delete_temp_file('test.txt')

assert content == 'Hello, World!'
assert not os.path.exists(file_path)

JUSTIFICATION:
----------------------------------------
This is a good test case because it demonstrates a universal pattern of file operations within a temporary directory, which is a common scenario in Python programming. The `delete_temp_file` method is a standard way to remove files, and the pattern of checking for file existence before removing it is widely recognized. The prompt establishes a clear context for the operations, and the completion follows well-established conventions, ensuring semantic clarity. The assertions verify the correctness of file creation, reading, and deletion, ensuring the model's understanding of the pattern.

ASSERTIONS:
----------------------------------------

assert not os.path.exists(file_path)
================================================================================

================================================================================
Test Case #23
================================================================================

================================================================================
Test Case ID: 23
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import json

def save_to_file(data, filename):
    with open(filename, 'w') as file:
        json.dump(data, file)

def load_from_file(filename):
    with open(filename, 'r') as file:


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        data = json.load(file)

SUFFIX CODE:
----------------------------------------
        return data

# Sample usage
data = {'key': 'value'}
filename = 'testfile.json'
save_to_file(data, filename)
loaded_data = load_from_file(filename)
assert loaded_data == data



JUSTIFICATION:
----------------------------------------
This test case is a good example of low-context pattern matching because it relies on the model's ability to recognize the standard pattern of using the `json` module to read and write data to files in Python. The pattern of `json.load` and `json.dump` is a universal, well-established convention in Python programming. The prompt and suffix provide enough context to suggest the pattern, and the completion must correctly follow this pattern to pass the assertions.

ASSERTIONS:
----------------------------------------
assert load_from_file('testfile.json') == {'key': 'value'}

================================================================================

================================================================================
Test Case #24
================================================================================

================================================================================
Test Case ID: 24
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import os

def read_file_content(file_path):
    try:

LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        with open(file_path, 'r') as file:
            return file.read()

SUFFIX CODE:
----------------------------------------

    except FileNotFoundError:
        return None
    except Exception as e:
        raise e

# Test cases
if __name__ == '__main__':
    # Create a sample file for testing
    test_file_path = 'test_file.txt'
    with open(test_file_path, 'w') as f:
        f.write('Sample content')

    # Assertions
    assert read_file_content(test_file_path) == 'Sample content'
    assert read_file_content('nonexistent_file.txt') is None



JUSTIFICATION:
----------------------------------------
This test case is designed to evaluate the model's ability to recognize and complete a common pattern for reading file content in Python. The context is minimal, yet the pattern of opening a file and reading its content is a widely-used and standardized practice. The completion must handle the file correctly and ensure that resources are properly managed. The assertions verify that the function behaves as expected with both existing and non-existing files.

ASSERTIONS:
----------------------------------------
assert read_file_content(test_file_path) == 'Sample content'
assert read_file_content('nonexistent_file.txt') is None
================================================================================

================================================================================
Test Case #25
================================================================================

================================================================================
Test Case ID: 25
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import os

def read_file(file_path):
    try:
        with open(file_path, 'r') as file:
            data = file.read()


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
            return data

SUFFIX CODE:
----------------------------------------
    except Exception as e:
        print(f"Error reading file: {e}")

# Test case
file_path = 'test.txt'
with open(file_path, 'w') as f:
    f.write('Hello, World!')

result = read_file(file_path)
os.remove(file_path)


JUSTIFICATION:
----------------------------------------
This example tests the model's ability to recognize and complete a common file reading pattern using Python's `with` statement for resource management. The code involves reading a file and handling potential exceptions, which is a universal pattern in Python programming. The prompt and suffix provide enough context to understand the typical use of `with open`, error handling, and returning the read data. The golden completion maintains this pattern and ensures proper cleanup and error handling, making this a good test case for low-context pattern matching.

ASSERTIONS:
----------------------------------------
assert result == 'Hello, World!'

================================================================================

================================================================================
Test Case #26
================================================================================

================================================================================
Test Case ID: 26
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import os

class FileHandler:
    def __init__(self, filepath):
        self.filepath = filepath

    def read_file(self):
        try:


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
            file = open(self.filepath, 'r')
            content = file.read()

SUFFIX CODE:
----------------------------------------
            return content
        finally:
            file.close()

# Testing the FileHandler class
handler = FileHandler('testfile.txt')
with open('testfile.txt', 'w') as f:
    f.write('Hello, World!')

result = handler.read_file()
assert result == 'Hello, World!'



JUSTIFICATION:
----------------------------------------
This test case involves a common file handling pattern where a file is opened, read, and then closed using a try/finally block. This pattern is widely used and follows standard conventions, making it a good candidate for testing low-context pattern recognition. The completion demonstrates the correct usage of opening a file and reading its content, while the assertions validate the behavior, ensuring the file content is read correctly. The cleanup code ensures no resources are left open or files left behind, maintaining good practice.

ASSERTIONS:
----------------------------------------
assert handler.read_file() == 'Hello, World!'

================================================================================

================================================================================
Test Case #27
================================================================================

================================================================================
Test Case ID: 27
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import json

class ConfigManager:
    def __init__(self, config_file):
        self.config_file = config_file
        self.config = None

    def load_config(self):
        with open(self.config_file, 'r') as file:

LLM COMPLETION gpt-4.5-preview:
----------------------------------------
            self.config = json.load(file)

SUFFIX CODE:
----------------------------------------


    def get_config_value(self, key):
        return self.config.get(key, None)


JUSTIFICATION:
----------------------------------------
This scenario tests the LLM's ability to recognize the common pattern of reading a JSON configuration file in Python. The prefix provides minimal context, requiring the LLM to infer the necessary completion based on the usage of the json module and file handling. The pattern of loading a JSON file is universal and follows a well-established convention, making it ideal for testing low-context pattern recognition.

ASSERTIONS:
----------------------------------------
import json
with open('test_config.json', 'w') as file:
    json.dump({'key1': 'value1', 'key2': 42}, file)

config_manager = ConfigManager('test_config.json')
config_manager.load_config()

assert config_manager.get_config_value('key1') == 'value1'
assert config_manager.get_config_value('key2') == 42


import os
os.remove('test_config.json')
================================================================================

================================================================================
Test Case #28
================================================================================

================================================================================
Test Case ID: 28
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import os
import tempfile

def create_temp_file(content):
    temp_file = tempfile.NamedTemporaryFile(delete=False)
    try:
        temp_file.write(content.encode())
        temp_file.close()
        return temp_file.name
    except Exception as e:
        temp_file.close()
        os.remove(temp_file.name)
        raise e

# Creating a temporary file with some content
file_path = create_temp_file('Hello, World!')
# Read and assert


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
with open(file_path, 'r') as f:
    assert f.read() == 'Hello, World!'

SUFFIX CODE:
----------------------------------------

# Cleanup
if os.path.exists(file_path):
    os.remove(file_path)

JUSTIFICATION:
----------------------------------------
This example tests the model's ability to recognize and complete a common pattern for file operations in Python, specifically reading from a file. The pattern of opening a file with 'with' statement and reading its content is a widely-used and standardized practice. The prompt sets up the context of creating a temporary file, and the completion should naturally follow with reading from that file. The assertions ensure that the file content is as expected, verifying that the pattern was correctly followed.

ASSERTIONS:
----------------------------------------
assert content == 'Hello, World!'
================================================================================

================================================================================
Test Case #29
================================================================================

================================================================================
Test Case ID: 29
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import os

def create_and_write_file(filename, content):
    try:
        with open(filename, 'w') as f:


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
            f.write(content)

SUFFIX CODE:
----------------------------------------
        os.remove(filename)
    except OSError as e:
        print(f"Error: {e}")

# Test the function
filename = 'testfile.txt'
content = 'Hello, world!'
create_and_write_file(filename, content)


JUSTIFICATION:
----------------------------------------
This is a good test case because it checks for the ability to recognize a common pattern of writing to a file with minimal context. The pattern of opening a file, writing content, and handling exceptions is universally used and follows well-established conventions in Python. The provided context is minimal but sufficient to identify the need to write to the file. The suffix includes cleanup code and exception handling, ensuring the resource is cleaned up after assertions.

ASSERTIONS:
----------------------------------------
assert os.path.exists(filename) == False

================================================================================

================================================================================
Test Case #30
================================================================================

================================================================================
Test Case ID: 30
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class FileManager:
    def __init__(self, file_name):
        self.file_name = file_name
        self.file = None

    def __enter__(self):
        self.file = open(self.file_name, 'r')
        return self.file



LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    def __exit__(self, exc_type, exc_value, traceback):
        if self.file:
            self.file.close()

SUFFIX CODE:
----------------------------------------


JUSTIFICATION:
----------------------------------------
This test case is a good example of a context manager and resource handling pattern in Python. The pattern is clear and identifiable with minimal context and follows widely-used conventions for managing file resources using the `with` statement. The completion demonstrates proper resource cleanup and error handling. The assertions validate that the file content is read correctly, and the cleanup code ensures the file is removed after the test.

ASSERTIONS:
----------------------------------------
with open('test.txt', 'w') as f:
    f.write('Hello, world!')

with FileManager('test.txt') as f:
    content = f.read()

assert content == 'Hello, world!'

================================================================================

================================================================================
Test Case #31
================================================================================

================================================================================
Test Case ID: 31
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class ResourceHandler:
    def __init__(self, resource_name):
        self.resource_name = resource_name
        self.resource = None

    def __enter__(self):
        self.resource = open(self.resource_name, 'w')
        return self.resource



LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    def __exit__(self, exc_type, exc_value, traceback):
        if self.resource:
            self.resource.close()

SUFFIX CODE:
----------------------------------------


with ResourceHandler('testfile.txt') as f:
    f.write('Hello, World!')

# Clean up
import os
os.remove('testfile.txt')


JUSTIFICATION:
----------------------------------------
This is a good test case because it involves context managers, which are a common resource handling pattern in Python. The pattern is recognizable with minimal context, requiring the completion to correctly close the resource and handle exceptions.

ASSERTIONS:
----------------------------------------
with ResourceHandler('testfile.txt') as f:
    f.write('Hello, World!')
assert os.path.exists('testfile.txt')
with open('testfile.txt', 'r') as f:
    assert f.read() == 'Hello, World!'
# Clean up
os.remove('testfile.txt')
================================================================================

================================================================================
Test Case #32
================================================================================

================================================================================
Test Case ID: 32
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
def process_text(text):
    # Convert the text to uppercase
    upper_text = text.upper()
    # Replace spaces with underscores


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    transformed_text = upper_text.replace(' ', '_')

SUFFIX CODE:
----------------------------------------

    return transformed_text

# Test cases
assert process_text('hello world') == 'HELLO_WORLD'
assert process_text('Python Programming') == 'PYTHON_PROGRAMMING'
assert process_text('Test Case') == 'TEST_CASE'

JUSTIFICATION:
----------------------------------------
This is a good test case because it involves a common string manipulation pattern where text is transformed by converting to uppercase and replacing spaces with underscores. The pattern is recognizable with minimal context and follows widely-used conventions. The completion demonstrates understanding of chaining string methods and maintaining the transformation order, which is essential for correct functionality.

ASSERTIONS:
----------------------------------------
assert process_text('hello world') == 'HELLO_WORLD'
assert process_text('Python Programming') == 'PYTHON_PROGRAMMING'
assert process_text('Test Case') == 'TEST_CASE'
================================================================================

================================================================================
Test Case #33
================================================================================

================================================================================
Test Case ID: 33
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class FileManager:
    def __init__(self, filename):
        self.filename = filename

    def __enter__(self):
        self.file = open(self.filename, 'r')
        return self.file


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    def __exit__(self, exc_type, exc_value, traceback):
        self.file.close()

SUFFIX CODE:
----------------------------------------


with open('example.txt', 'w') as f:
    f.write('This is a test file.')

with FileManager('example.txt') as f:
    content = f.read()



JUSTIFICATION:
----------------------------------------
This is a good test case because it checks the model's ability to recognize and complete the context manager pattern in Python. The pattern of implementing `__enter__` and `__exit__` methods is widely used and recognizable with minimal context. The completion must ensure that resources are properly managed, which is critical in real-world applications. The assertions verify that the file was correctly opened and read, and the resource management was handled as expected.

ASSERTIONS:
----------------------------------------
assert content == 'This is a test file.'

import os
os.remove('example.txt')

================================================================================

================================================================================
Test Case #34
================================================================================

================================================================================
Test Case ID: 34
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class MyClass:
    def __init__(self, value):
        self.value = value

    def increment(self, amount):
        self.value += amount

    def display_value(self):
        print(f'Value: {self.value}')

LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    def reset_value(self):
        self.value = 0

SUFFIX CODE:
----------------------------------------


# Creating an instance of MyClass
obj = MyClass(10)
# Incrementing the value
obj.increment(5)
# Displaying the current value
obj.display_value()

# Resetting the value to 0
obj.reset_value()

# Displaying the reset value
obj.display_value()

JUSTIFICATION:
----------------------------------------
This test case evaluates the LLM's ability to recognize and continue an object-oriented pattern by adding a new method to a class. The completion must maintain consistency with the class's method definitions and should properly manipulate the class attribute. The pattern of defining and adding methods to a class is a common practice in OOP, and the LLM should be able to handle it with minimal context.

ASSERTIONS:
----------------------------------------
obj.reset_value()
assert obj.value == 0
obj.increment(10)
assert obj.value == 10
obj.reset_value()
assert obj.value == 0

# Reset value test
obj.value = 15
obj.reset_value()
assert obj.value == 0

================================================================================

================================================================================
Test Case #35
================================================================================

================================================================================
Test Case ID: 35
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import threading

class Counter:
    def __init__(self):
        self.count = 0
        self.lock = threading.Lock()

    def increment(self):
        with self.lock:


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
            self.count += 1

SUFFIX CODE:
----------------------------------------

        return self.count

counter = Counter()
threads = []
for _ in range(10):
    t = threading.Thread(target=counter.increment)
    threads.append(t)
    t.start()
for t in threads:
    t.join()

assert counter.count == 10, f"Expected count to be 10, but got {counter.count}"


JUSTIFICATION:
----------------------------------------
This example tests the LLM's ability to recognize and complete the pattern of thread-safe data structure manipulation using context managers and resource handling. The prompt establishes a clear pattern of using a lock within a context manager to safely increment a counter, and the golden completion requires the LLM to understand and follow this pattern. The suffix includes assertions to verify the correctness of the implementation and ensure the counter is incremented correctly in a multithreaded environment.

ASSERTIONS:
----------------------------------------
assert counter.count == 10, f"Expected count to be 10, but got {counter.count}"
================================================================================

================================================================================
Test Case #36
================================================================================

================================================================================
Test Case ID: 36
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class FileHandler:
    def __init__(self, filename):
        self.filename = filename

    def __enter__(self):
        self.file = open(self.filename, 'r')
        return self.file

    def __exit__(self, exc_type, exc_val, exc_tb):


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        self.file.close()

SUFFIX CODE:
----------------------------------------


# Usage example
def read_first_line(filename):
    with FileHandler(filename) as f:
        return f.readline()



JUSTIFICATION:
----------------------------------------
This example tests the LLM's ability to recognize and complete a context manager pattern in Python, focusing on proper resource handling and cleanup in the __exit__ method. The completion must correctly handle the file resource and return appropriate values, demonstrating an understanding of the context manager protocol with minimal surrounding context.

ASSERTIONS:
----------------------------------------
import os
with open('testfile.txt', 'w') as f:
    f.write('First line of the file\nSecond line\n')
assert read_first_line('testfile.txt') == 'First line of the file\n'
os.remove('testfile.txt')
================================================================================

================================================================================
Test Case #37
================================================================================

================================================================================
Test Case ID: 37
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class MyClass:
    def __init__(self, value):
        self.value = value

    def add(self, x):
        return self.value + x

    def multiply(self, x):
        return self.value * x

    def subtract(self, x):

LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        return self.value - x

SUFFIX CODE:
----------------------------------------


# Test cases
obj = MyClass(10)
assert obj.add(5) == 15
assert obj.multiply(2) == 20
assert obj.subtract(3) == 7

print("All tests passed!")

JUSTIFICATION:
----------------------------------------
This is a good test case because it requires the LLM to recognize the pattern of class method definitions and correctly complete the 'subtract' method based on the context provided by the 'add' and 'multiply' methods. The pattern involves performing arithmetic operations on 'self.value' with the parameter 'x'. The completion demonstrates understanding of this pattern by implementing the 'subtract' method to return the result of subtracting 'x' from 'self.value'.

ASSERTIONS:
----------------------------------------
obj = MyClass(10)
assert obj.add(5) == 15
assert obj.multiply(2) == 20
assert obj.subtract(3) == 7
================================================================================

================================================================================
Test Case #38
================================================================================

================================================================================
Test Case ID: 38
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class DataProcessor:
    def __init__(self, data):
        self.data = data

    def filter_data(self, threshold):
        '''Filters data elements greater than the threshold'''


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        return [x for x in self.data if x > threshold]

SUFFIX CODE:
----------------------------------------

    def process_data(self):
        '''Processes data by filtering and then sorting it'''
        self.filtered_data = self.filter_data(10)
        self.sorted_data = sorted(self.filtered_data)

# Test the DataProcessor class
processor = DataProcessor([5, 20, 15, 10, 25, 7])
processor.process_data()

# Assertions
assert processor.filtered_data == [20, 15, 25]
assert processor.sorted_data == [15, 20, 25]


JUSTIFICATION:
----------------------------------------
This test case is good because it evaluates the LLM's ability to recognize and continue a pattern of filtering data within a class method. The prompt provides minimal context but establishes a clear pattern of data processing within class methods. The golden completion involves filtering data based on a threshold, a common and universal programming task, and the assertions validate the correct functionality of the implemented method.

ASSERTIONS:
----------------------------------------
assert DataProcessor([5, 20, 15, 10, 25, 7]).filter_data(10) == [20, 15, 25]
assert DataProcessor([1, 2, 3, 4, 5]).filter_data(3) == [4, 5]

================================================================================

================================================================================
Test Case #39
================================================================================

================================================================================
Test Case ID: 39
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class StringProcessor:
    def __init__(self, text):
        self.text = text

    def process_text(self):
        # Convert text to uppercase
        self.text = self.text.upper()

    def extract_vowels(self):
        vowels = 'AEIOU'
        return ''.join([char for char in self.text if char in vowels])

    def __str__(self):
        return self.text

# Example usage:
processor = StringProcessor("hello world")
processor.process_text()
# Extract vowels


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
vowels = processor.extract_vowels()

SUFFIX CODE:
----------------------------------------

assert processor.text == "HELLO WORLD"
vowels = processor.extract_vowels()
assert vowels == "EOO"

# Clean up


JUSTIFICATION:
----------------------------------------
This test case evaluates the LLM's ability to recognize and complete a pattern involving object-oriented programming, specifically class methods and string manipulation. The example establishes a clear context for class instantiation, method calling, and string processing. The LLM must infer the logical next step to call the `extract_vowels` method and verify the result, demonstrating an understanding of class method usage and return value handling.

ASSERTIONS:
----------------------------------------
assert processor.text == "HELLO WORLD"
vowels = processor.extract_vowels()
assert vowels == "EOO"

================================================================================

================================================================================
Test Case #40
================================================================================

================================================================================
Test Case ID: 40
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class MyClass:
    def __init__(self, value):
        self.value = value

    def double_value(self):


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        return self.value * 2

SUFFIX CODE:
----------------------------------------


# Testing the MyClass
a = MyClass(10)
assert a.double_value() == 20
a.value = 5
assert a.double_value() == 10


JUSTIFICATION:
----------------------------------------
This example tests the LLM's ability to recognize and complete an object-oriented pattern involving a class method. The pattern is identifiable with minimal context and is universally understood. The method 'double_value' should return twice the value of 'self.value'. This tests the model's understanding of basic class method implementation.

ASSERTIONS:
----------------------------------------
a.value = 10
assert a.double_value() == 20
a.value = 5
assert a.double_value() == 10
================================================================================

================================================================================
Test Case #41
================================================================================

================================================================================
Test Case ID: 41
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class Animal:
    def __init__(self, name):
        self.name = name

    def speak(self):
        raise NotImplementedError('Subclasses must implement this method')

class Dog(Animal):


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    def speak(self):
        return f'Woof, I am {self.name}'

SUFFIX CODE:
----------------------------------------

dog = Dog('Buddy')
assert dog.speak() == 'Woof, I am Buddy'


JUSTIFICATION:
----------------------------------------
This example tests the model's ability to recognize and complete an object-oriented pattern involving class inheritance and method overriding. The prompt establishes a base class with an abstract method, and the suffix sets up an assertion that requires the subclass to implement the method correctly. The pattern of defining a method in a subclass to override a base class method is common and should be recognizable with minimal context.

ASSERTIONS:
----------------------------------------
dog = Dog('Buddy')
assert dog.speak() == 'Woof, I am Buddy'

================================================================================

================================================================================
Test Case #42
================================================================================

================================================================================
Test Case ID: 42
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class DataProcessor:
    def __init__(self, data):
        self.data = data

    def process_data(self):
        result = []
        for item in self.data:
            if self.is_valid(item):
                result.append(self.transform(item))
        return result

    def is_valid(self, item):
        # Custom validation logic


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        return item.isalpha()

SUFFIX CODE:
----------------------------------------

    def transform(self, item):
        # Custom transformation logic
        return item.upper()

# Example usage
processor = DataProcessor(['a', 'b', 'c', '1', '2'])
processed_data = processor.process_data()

assert processed_data == ['A', 'B', 'C']


JUSTIFICATION:
----------------------------------------
This test case evaluates the LLM's ability to recognize and complete a pattern within an object-oriented context. The 'is_valid' method needs to implement a validation check that aligns with the given data processing pattern. The prompt and suffix provide enough context to understand that 'is_valid' should filter alphabetic characters only, which is a common string validation check.

ASSERTIONS:
----------------------------------------
processor = DataProcessor(['a', 'b', 'c', '1', '2'])
processed_data = processor.process_data()
assert processed_data == ['A', 'B', 'C']
================================================================================

================================================================================
Test Case #43
================================================================================

================================================================================
Test Case ID: 43
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class Animal:
    def __init__(self, name):
        self.name = name

    def speak(self):
        raise NotImplementedError("Subclasses must implement this method")

class Dog(Animal):
    def speak(self):
        return "Woof!"

class Cat(Animal):
    def speak(self):
        return "Meow!"

# Adding a new subclass
class Parrot(Animal):


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    def speak(self):
        return "Squawk!"

SUFFIX CODE:
----------------------------------------


def test_animal_sounds():
    animals = [Dog('Buddy'), Cat('Whiskers'), Parrot('Polly')]
    sounds = [animal.speak() for animal in animals]
    assert sounds == ["Woof!", "Meow!", "Squawk!"]

# Run the test
test_animal_sounds()


JUSTIFICATION:
----------------------------------------
This is a good test case because it evaluates the model's ability to recognize and complete the pattern of subclassing and method overriding in object-oriented programming. The prompt provides enough context by showing two subclasses (Dog and Cat) that override the speak method, establishing a clear pattern. The suffix includes a test function that verifies the correct behavior of the added subclass (Parrot). Multiple valid completions are possible, but they must adhere to the established pattern of overriding the speak method to provide a unique sound for each animal.

ASSERTIONS:
----------------------------------------
assert Parrot('Polly').speak() == 'Squawk!'
test_animal_sounds()

================================================================================

================================================================================
Test Case #44
================================================================================

================================================================================
Test Case ID: 44
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class MyClass:
    def __init__(self, value):
        self.value = value

    def increment(self):


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        self.value += 1

SUFFIX CODE:
----------------------------------------


# Test the MyClass implementation
obj = MyClass(10)
obj.increment()
assert obj.value == 11
obj.increment()
assert obj.value == 12

JUSTIFICATION:
----------------------------------------
This test case is designed to evaluate the model's ability to recognize and complete a common object-oriented pattern: defining methods within a class. The prompt provides the class definition and method signature, with minimal context suggesting that the increment method should modify the instance's value attribute. Multiple valid completions could include various ways of incrementing the value, such as self.value += 1 or self.value = self.value + 1. The provided assertions test the correctness of the increment method and validate the pattern.

ASSERTIONS:
----------------------------------------
assert 'MyClass' in globals()
assert callable(MyClass(0).increment)
obj = MyClass(5)
obj.increment()
assert obj.value == 6
obj.increment()
assert obj.value == 7
================================================================================

================================================================================
Test Case #45
================================================================================

================================================================================
Test Case ID: 45
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
class Calculator:
    def add(self, a, b):
        return a + b

    def subtract(self, a, b):
        return a - b

    def multiply(self, a, b):
        return a * b

    def divide(self, a, b):
        # Complete the divide method


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
        if b == 0:
            raise ZeroDivisionError("division by zero")
        return a / b

SUFFIX CODE:
----------------------------------------

# Test cases
calc = Calculator()
assert calc.add(2, 3) == 5
assert calc.subtract(5, 3) == 2
assert calc.multiply(2, 3) == 6
assert calc.divide(6, 3) == 2
assert calc.divide(5, 2) == 2.5
try:
    calc.divide(5, 0)
except ZeroDivisionError:
    pass


JUSTIFICATION:
----------------------------------------
This is a good test case because it requires the model to recognize the pattern of basic arithmetic operations and correctly implement the division method, including proper error handling for division by zero. The prompt provides sufficient context for recognizing the pattern, and the suffix includes comprehensive assertions to verify the implementation.

ASSERTIONS:
----------------------------------------
assert calc.divide(6, 3) == 2
assert calc.divide(5, 2) == 2.5
try:
    calc.divide(5, 0)
except ZeroDivisionError:
    pass
================================================================================

================================================================================
Test Case #46
================================================================================

================================================================================
Test Case ID: 46
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools
import time

# A decorator for measuring the execution time of functions

def timing_decorator(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"{func.__name__} took {end_time - start_time:.4f} seconds")
        return result
    return wrapper

# Example function to be decorated
@timing_decorator
def example_function(x, y):
    time.sleep(0.1)
    return x + y

# Another function to be decorated
@timing_decorator
def another_function(a, b):
    time.sleep(0.2)
    return a * b

# Test the decorated functions
result1 = example_function(5, 3)
result2 = another_function(2, 4)

# Assertions to check the correctness of the decorated functions


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
assert result1 == 8
assert result2 == 8

SUFFIX CODE:
----------------------------------------
assert result1 == 8
assert result2 == 8

# Clean up resources (if any)
# In this example, there are no resources to clean up.


JUSTIFICATION:
----------------------------------------
This is a good test case because it establishes a common decorator pattern (timing_decorator) and applies it to multiple functions. The pattern is clear and recognizable with minimal context. The assertions check the correctness of the decorated functions and also ensure that the original function names are preserved, demonstrating proper use of functools.wraps in the decorator. Multiple valid completions are possible, as long as the checks for function names and results are included.

ASSERTIONS:
----------------------------------------
assert result1 == 8
assert result2 == 8

================================================================================

================================================================================
Test Case #47
================================================================================

================================================================================
Test Case ID: 47
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools
import time

# Decorator to measure execution time of a function

def timing_decorator(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        execution_time = end_time - start_time
        print(f"Executed {func.__name__} in {execution_time:.4f} seconds")
        return result
    return wrapper

# Example function to decorate
@timing_decorator
def compute_square_sum(n):
    """Compute the sum of squares of numbers from 1 to n"""
    return sum(i * i for i in range(1, n + 1))

@timing_decorator
def compute_cube_sum(n):
    """Compute the sum of cubes of numbers from 1 to n"""
    return sum(i * i * i for i in range(1, n + 1))

@timing_decorator
def compute_sum(n):
    """Compute the sum of numbers from 1 to n"""


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
    return sum(range(1, n + 1))

SUFFIX CODE:
----------------------------------------
# Test cases
result1 = compute_square_sum(10)
result2 = compute_cube_sum(10)
result3 = compute_sum(10)

assert result1 == 385, f"Expected 385 but got {result1}"
assert result2 == 3025, f"Expected 3025 but got {result2}"
assert result3 == 55, f"Expected 55 but got {result3}"


JUSTIFICATION:
----------------------------------------
This test case provides a clear example of a decorator pattern in Python, specifically a timing decorator. The prefix establishes the pattern of using the timing_decorator to measure execution time of functions. The missing completion is the body of the compute_sum function, which should follow the same pattern as compute_square_sum and compute_cube_sum. The golden_completion demonstrates understanding of the pattern with minimal context, ensuring the function is correctly implemented. The assertions verify correctness of the decorated functions.

ASSERTIONS:
----------------------------------------
assert compute_square_sum(10) == 385
assert compute_cube_sum(10) == 3025
assert compute_sum(10) == 55

================================================================================

================================================================================
Test Case #48
================================================================================

================================================================================
Test Case ID: 48
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools
from collections import namedtuple

class Event:
    def __init__(self):
        self._callbacks = []

    def register(self, callback):
        self._callbacks.append(callback)

    def unregister(self, callback):
        self._callbacks.remove(callback)

    def fire(self, *args, **kwargs):
        for callback in self._callbacks:
            callback(*args, **kwargs)

class Button:
    def __init__(self):
        self.on_click = Event()

button = Button()

EventData = namedtuple('EventData', ['type', 'button'])

@functools.lru_cache()
def handle_click(event):
    print(f'Button clicked with event: {event}')

# Register the callback
button.on_click.register(handle_click)

# Create event data as a namedtuple
event_data = EventData(type='click', button='left')


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
button.on_click.fire(event_data)

SUFFIX CODE:
----------------------------------------
button.on_click.fire(event_data)


JUSTIFICATION:
----------------------------------------
This example tests the LLM's ability to complete an event handling pattern. The prefix sets up a button click event system with a registered callback function. The golden completion requires recognizing the pattern of firing an event to trigger the callback, which should print a message. The suffix then tests unregistering the callback and ensuring that firing the event no longer triggers the callback, demonstrating proper event handling and resource cleanup.

ASSERTIONS:
----------------------------------------
assert handle_click.cache_info().hits == 1, 'Callback should have been called once'
button.on_click.unregister(handle_click)
button.on_click.fire(event_data)
assert handle_click.cache_info().hits == 1, 'Callback should not be called after unregistration'
================================================================================

================================================================================
Test Case #49
================================================================================

================================================================================
Test Case ID: 49
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools

def logger(func):
    """A decorator that logs the function call details - arguments and return value."""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        args_repr = [repr(a) for a in args]                      # 1
        kwargs_repr = [f"{k}={v!r}" for k, v in kwargs.items()]  # 2
        signature = ", ".join(args_repr + kwargs_repr)          # 3
        print(f"Calling {func.__name__}({signature})")
        value = func(*args, **kwargs)
        print(f"{func.__name__} returned {value!r}")            # 4
        return value
    return wrapper

@logger
def say_hello(name):
    return f"Hello, {name}!"

@logger


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
def calculate_sum(a, b):
    return a + b

SUFFIX CODE:
----------------------------------------
# Testing the decorated functions
assert say_hello("Alice") == "Hello, Alice!"
assert calculate_sum(3, 4) == 7


JUSTIFICATION:
----------------------------------------
This test case evaluates the ability of an LLM to recognize and complete a common decorator pattern in Python. The prefix establishes a logging decorator and shows its application on a sample function, `say_hello`. The completion should apply the same decorator to another function `calculate_sum` following the same pattern. The suffix contains tests verifying the correctness of both decorated functions, making it a good test for low-context pattern recognition.

ASSERTIONS:
----------------------------------------
assert say_hello("Alice") == "Hello, Alice!"
assert calculate_sum(3, 4) == 7

================================================================================

================================================================================
Test Case #50
================================================================================

================================================================================
Test Case ID: 50
Source: devbench-low-context
Language: python
================================================================================

PREFIX CODE:
----------------------------------------
import functools
import time

# Decorator pattern example with timing functionality
class TimedCache:
    def __init__(self, timeout=60):
        self.cache = {}
        self.timeout = timeout

    def __call__(self, func):
        @functools.wraps(func)
        def wrapped(*args, **kwargs):
            key = (args, tuple(sorted(kwargs.items())))
            if key in self.cache:
                result, timestamp = self.cache[key]
                if time.time() - timestamp < self.timeout:
                    return result
            result = func(*args, **kwargs)
            self.cache[key] = (result, time.time())
            return result
        return wrapped

def slow_function(x, y):
    time.sleep(2)
    return x + y

# Applying the decorator


LLM COMPLETION gpt-4.5-preview:
----------------------------------------
cached_slow_function = TimedCache(timeout=60)(slow_function)

SUFFIX CODE:
----------------------------------------

# Testing the cached function
start_time = time.time()
result1 = cached_slow_function(2, 3)
elapsed1 = time.time() - start_time

start_time = time.time()
result2 = cached_slow_function(2, 3)
elapsed2 = time.time() - start_time

assert result1 == 5
assert result2 == 5
assert elapsed1 >= 2
assert elapsed2 < 2

print("All assertions passed.")

JUSTIFICATION:
----------------------------------------
This test case checks the LLM's ability to recognize and complete the decorator pattern in Python. The prefix sets up a caching decorator and applies it to a slow function. The golden completion involves reapplying the decorator to the function, which is a common pattern. The suffix includes test code to verify the caching functionality, ensuring that the decorator works as intended and that the function result is cached correctly. This scenario tests the model's understanding of decorators, function wrapping, and caching mechanisms, which are key aspects of Python metaprogramming.

ASSERTIONS:
----------------------------------------
assert result1 == 5
assert result2 == 5
assert elapsed1 >= 2
assert elapsed2 < 2

================================================================================
