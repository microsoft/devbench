{"id": "1", "testsource": "synthbench-low-context", "language": "python", "prefix": "import os\n\ndef read_file(file_path):\n    try:", "suffix": "\n    finally:\n        if os.path.exists(file_path):\n            os.remove(file_path)\n\n# Test cases\ntry:\n    with open('test.txt', 'w') as f:\n        f.write('Hello, world!')\n    content = read_file('test.txt')\n    assert content == 'Hello, world!'\nexcept Exception as e:\n    assert False, f'Exception occurred: {e}'", "golden_completion": "\n        with open(file_path, 'r') as file:\n            return file.read()\n    except FileNotFoundError:\n        return ''", "LLM_justification": "This test case demonstrates a common pattern of file handling in Python, which involves opening a file, reading its contents, and handling the potential FileNotFoundError. The completion should recognize the need to read the file content and handle exceptions properly. The scenario is designed to test the LLM's ability to identify and continue a standard file I/O pattern with minimal context.", "assertions": "assert read_file('non_existent.txt') == '', 'Should return empty string if file does not exist'\ntry:\n    with open('example.txt', 'w') as f:\n        f.write('Example content')\n    content = read_file('example.txt')\n    assert content == 'Example content', 'Content should match written content'\nexcept Exception as e:\n    assert False, f'Exception occurred: {e}'\nfinally:\n    if os.path.exists('example.txt'):\n        os.remove('example.txt')", "gpt-4.1-mini": "        with open(file_path, 'r') as f:\n            return f.read()"}
{"id": "2", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\nimport time\n\ndef timer(func):\n    \"\"\"Print the runtime of the decorated function\"\"\"\n    @functools.wraps(func)\n    def wrapper_timer(*args, **kwargs):\n        start_time = time.perf_counter()\n        value = func(*args, **kwargs)\n        end_time = time.perf_counter()\n        run_time = end_time - start_time\n        print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\n        return value\n    return wrapper_timer\n\n@timer\ndef waste_some_time(num_times):\n    for _ in range(num_times):\n        sum([i**2 for i in range(10000)])\n\n@timer\ndef do_something():\n    print(\"Doing something important...\")\n", "suffix": "do_something()\n\n@timer\ndef another_function():\n    time.sleep(1)\n    return \"Done\"\n\nresult = another_function()\nassert result == \"Done\"\n", "golden_completion": "waste_some_time(5)\n", "LLM_justification": "This example tests the LLM's ability to recognize and complete the decorator pattern. The prefix provides a detailed setup of the timer decorator, which measures the execution time of the decorated functions. The suffix demonstrates the usage of the timer decorator on a different function and includes assertions to verify the correctness of another function. The completion must correctly use the established timer decorator pattern with minimal context provided.", "assertions": "waste_some_time(5)\n", "gpt-4.1-mini": "do_something()"}
{"id": "3", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\n\ndef log_execution(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f'Executing {func.__name__}...')\n        result = func(*args, **kwargs)\n        print(f'{func.__name__} executed.')\n        return result\n    return wrapper\n\n@log_execution\n", "suffix": "\n\nresult = add(3, 5)\n\nassert result == 8\n", "golden_completion": "def add(a, b):\n    return a + b\n\n", "LLM_justification": "This test case assesses the LLM's ability to recognize and continue a decorator pattern in Python. The prefix sets up a common logging decorator, and the completion should involve applying this decorator to a simple function. The suffix differs from the completion to ensure the test case evaluates understanding of pattern continuation, not just copy-pasting.", "assertions": "assert result == 8\n", "gpt-4.1-mini": "def add(a, b):\n    return a + b"}
{"id": "4", "testsource": "synthbench-low-context", "language": "python", "prefix": "from functools import wraps\nimport time\n\n# Decorator to measure the execution time of a function\n\ndef timeit(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        print(f\"Function {func.__name__} took {end_time - start_time:.4f} seconds\")\n        return result\n    return wrapper\n\n@timeit\ndef slow_function(seconds):\n    time.sleep(seconds)\n    return f\"Slept for {seconds} seconds\"\n\n@timeit\ndef fast_function():\n    return \"Done quickly\"\n\n# Testing the decorators\nresult_slow = slow_function(2)\nresult_fast = fast_function()\n# Check the results\n", "suffix": "# Clean up resources if any (not needed in this example)\nprint(\"All assertions passed.\")\n", "golden_completion": "assert 'Slept for 2 seconds' in result_slow\nassert result_fast == 'Done quickly'\n", "LLM_justification": "This test case is a good example of decorator usage, specifically for measuring execution time. The pattern is clear and well-established, and the prefix provides comprehensive context for the decorator's implementation. The golden completion tests the decorator's functionality, while the suffix provides different code to avoid duplication. Assertions ensure the decorator works as expected, and the test case covers multiple valid completions.", "assertions": "assert 'Slept for 2 seconds' in result_slow\nassert result_fast == 'Done quickly'\n", "gpt-4.1-mini": "assert result_slow == \"Slept for 2 seconds\"\nassert result_fast == \"Done quickly\""}
{"id": "5", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\n\ndef trace(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f'TRACE: calling {func.__name__}() with {args}, {kwargs}')\n        result = func(*args, **kwargs)\n        print(f'TRACE: {func.__name__}() returned {result}')\n        return result\n    return wrapper\n\nclass Calculator:\n    def __init__(self):\n        self.value = 0\n\n    @trace\n    def add(self, amount):\n        self.value += amount\n        return self.value\n\n    @trace\n    def subtract(self, amount):\n        self.value -= amount\n        return self.value\n\n    # Missing method implementation\n", "suffix": "\n    @trace\n    def divide(self, amount):\n        if amount == 0:\n            print('Cannot divide by zero')\n            return\n        self.value /= amount\n        return self.value\n\ncalc = Calculator()\ncalc.add(10)\ncalc.subtract(5)\ncalc.divide(2)\nassert calc.value == 2.5\ncalc.divide(0)  # This should raise an exception\n", "golden_completion": "\n    @trace\n    def multiply(self, amount):\n        self.value *= amount\n        return self.value\n", "LLM_justification": "This test case is a good example for evaluating an LLM's ability to recognize and complete a pattern involving decorator usage in a class. The prefix provides a clear pattern of methods within the Calculator class being decorated with a trace decorator, and the missing implementation of the multiply method follows the same pattern. The suffix provides a different method implementation (divide) to ensure that the completion is distinct. The assertions check for correct functionality and error handling, ensuring that the pattern is correctly followed.", "assertions": "calc = Calculator()\ncalc.add(10)\nassert calc.value == 10\ncalc.multiply(2)\nassert calc.value == 20\ncalc.subtract(5)\nassert calc.value == 15\ncalc.divide(3)\nassert calc.value == 5.0\ntry:\n    calc.divide(0)\nexcept ValueError as e:\n    assert str(e) == 'Cannot divide by zero'", "gpt-4.1-mini": "    def divide(self, amount):\n        if amount == 0:\n            raise ValueError('Cannot divide by zero')\n        self.value /= amount\n        return self.value"}
{"id": "6", "testsource": "synthbench-low-context", "language": "python", "prefix": "import time\nimport threading\n\nclass EventManager:\n    def __init__(self):\n        self.listeners = []\n\n    def register_listener(self, listener):\n        self.listeners.append(listener)\n\n    def trigger_event(self, event_data):\n        for listener in self.listeners:\n            listener(event_data)\n\n# A sample listener function\n\ndef sample_listener(event_data):\n    print(f\"Event received with data: {event_data}\")\n\n# Instantiate the EventManager and register the listener\nmanager = EventManager()\nmanager.register_listener(sample_listener)\n\n# Function to simulate event trigger\ndef simulate_event_trigger(manager):\n    time.sleep(1)\n    manager.trigger_event(\"Test Event\")\n\n# Start the event simulation in a separate thread\nthread = threading.Thread(target=simulate_event_trigger, args=(manager,))\nthread.start()\n", "suffix": "# Another listener function\n\ndef another_listener(event_data):\n    print(f\"Another event received with data: {event_data}\")\n\n# Register another listener and trigger another event\nmanager.register_listener(another_listener)\nsimulate_event_trigger(manager)\n\nassert 'sample_listener' in [listener.__name__ for listener in manager.listeners]\nassert 'another_listener' in [listener.__name__ for listener in manager.listeners]\n", "golden_completion": "thread.join()\n", "LLM_justification": "This example tests the model's ability to recognize and complete an event handling pattern. The prefix sets up an event manager, registers a listener, and simulates an event trigger in a separate thread. The golden completion, joining the thread, is crucial for ensuring that the event handling completes before proceeding. The suffix registers another listener and triggers another event, checking that both listeners are registered correctly.", "assertions": "assert 'sample_listener' in [listener.__name__ for listener in manager.listeners]\nassert 'another_listener' in [listener.__name__ for listener in manager.listeners]\n", "gpt-4.1-mini": "thread.join()"}
{"id": "7", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\nimport time\n\n# This is a decorator to log the execution time of functions\n\ndef log_execution_time(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        print(f\"{func.__name__} took {end_time - start_time:.4f} seconds\")\n        return result\n    return wrapper\n\n# Example function to demonstrate the decorator\n\n@log_execution_time\ndef example_function(n):\n    \"\"\" Function that sleeps for n seconds \"\"\"\n    time.sleep(n)\n    return f\"Slept for {n} seconds\"\n\n# Another function that should use the same decorator\n@log_execution_time\ndef another_example_function():\n    \"\"\" Function that performs a quick task \"\"\"\n", "suffix": "    return \"Quick task completed\"\n\n# Assertions to test the functionality of the decorator\nresult = example_function(2)\nassert result == \"Slept for 2 seconds\"\n\nresult = another_example_function()\nassert result == \"Quick task completed\"\n\n# Cleanup code (if any) would go here\n", "golden_completion": "    time.sleep(0.1)\n", "LLM_justification": "This test case evaluates an LLM's ability to recognize and complete a decorator pattern in Python with minimal context. The prefix establishes the pattern by defining a decorator function and applying it to `example_function`. The golden completion requires the LLM to continue this pattern by correctly implementing the body of `another_example_function` to include a quick task. The suffix validates the pattern adherence with assertions. This scenario tests the LLM's understanding of decorators and their application, ensuring it can recognize and extend patterns based on limited context.", "assertions": "result = example_function(2)\nassert result == \"Slept for 2 seconds\"\n\nresult = another_example_function()\nassert result == \"Quick task completed\"\n", "gpt-4.1-mini": "    return \"Quick task completed\""}
{"id": "8", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\n\nclass EventEmitter:\n    def __init__(self):\n        self.listeners = {}\n\n    def on(self, event, listener):\n        if event not in self.listeners:\n            self.listeners[event] = []\n        self.listeners[event].append(listener)\n\n    def emit(self, event, *args, **kwargs):\n        if event in self.listeners:\n            for listener in self.listeners[event]:\n                listener(*args, **kwargs)\n\nemitter = EventEmitter()\n\n# Register a callback for the 'data' event\nemitter.on('data', lambda x: print(f'Data received: {x}'))\n\n# Register a callback for the 'error' event\n", "suffix": "# Emit 'error' event\nemitter.emit('error', 'Something went wrong')\n# Emit 'data' event\nemitter.emit('data', 42)\n\n", "golden_completion": "emitter.on('error', lambda e: print(f'Error: {e}'))", "LLM_justification": "This test case examines the LLM's ability to recognize and complete a common callback registration pattern in an event handling system. The pattern involves registering callbacks for specific events and then emitting those events. The prefix sets up the EventEmitter class and registers a callback for a 'data' event. The golden completion requires recognizing that another callback should be registered for the 'error' event, following the same pattern. The suffix ensures that different events are emitted to test the callbacks.", "assertions": "emitter.emit('error', 'Something went wrong')\nemitter.emit('data', 42)\nassert emitter.listeners['data'][0]('Test data') is None\nassert emitter.listeners['error'][0]('Test error') is None\n\n# Cleanup\nemitter.listeners.clear()", "gpt-4.1-mini": "emitter.on('error', lambda msg: print(f'Error: {msg}'))"}
{"id": "9", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\n\ndef log_execution(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f'Calling {func.__name__} with args: {args} and kwargs: {kwargs}')\n        result = func(*args, **kwargs)\n        print(f'{func.__name__} returned {result}')\n        return result\n    return wrapper\n\n@log_execution\ndef add(a, b):\n    return a + b\n\n@log_execution\n", "suffix": "def multiply(a, b):\n    return a * b\n\nresult_add = add(2, 3)\nresult_multiply = multiply(2, 3)\n\nassert result_add == 5\nassert result_multiply == 6\n", "golden_completion": "def subtract(a, b):\n    return a - b\n", "LLM_justification": "This example tests the model's ability to recognize and complete a decorator pattern. The prefix establishes a logging decorator and applies it to an `add` function. The completion should define another function, `subtract`, following the same pattern of being decorated with `@log_execution`. This is a good test case because it requires the model to recognize the decorator pattern and apply it consistently with minimal context.", "assertions": "result_subtract = subtract(5, 2)\nassert result_subtract == 3", "gpt-4.1-mini": "def multiply(a, b):"}
{"id": "10", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\n\nclass EventHandler:\n    def __init__(self):\n        self._callbacks = []\n\n    def register_callback(self, func):\n        self._callbacks.append(func)\n\n    def handle_event(self, *args, **kwargs):\n        for callback in self._callbacks:\n            callback(*args, **kwargs)\n\nclass Logger:\n    def __init__(self, name):\n        self.logger = logging.getLogger(name)\n\n    def log(self, message):\n        self.logger.debug(message)\n\nhandler = EventHandler()\nlogger = Logger('testLogger')\n\n# Decorator to log function calls\n", "suffix": "handler.register_callback(process_event)\nhandler.register_callback(logger.log)\n\nhandler.handle_event('Event occurred!')\n\n", "golden_completion": "def log_function_call(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        logger.log(f'Calling {func.__name__} with args {args} and kwargs {kwargs}')\n        return func(*args, **kwargs)\n    return wrapper\n\n@log_function_call\ndef process_event(event_data):\n    print(f\"Processing event: {event_data}\")\n    return event_data\n", "LLM_justification": "This test case evaluates the LLM's ability to recognize and complete a decorator pattern within the context of callback and event handling. The prefix sets up an event handler and a logger, introducing the need for a decorator to log function calls. The completion should correctly implement a logging decorator and register it as a callback. This tests the LLM's understanding of decorators, logging, and event handling patterns.", "assertions": "assert len(handler._callbacks) == 2\nassert handler._callbacks[1].__name__ == 'log'\nassert handler._callbacks[1].__self__.logger.name == 'testLogger'\n", "gpt-4.1-mini": "def log_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        logger.log(f\"Calling function {func.__name__}\")\n        return func(*args, **kwargs)\n    return wrapper\n\n@log_decorator\ndef process_event(event):\n    pass"}
{"id": "11", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\n\nclass EventEmitter:\n    def __init__(self):\n        self._events = {}\n\n    def on(self, event, callback=None):\n        # When used as a decorator, only event is provided\n        if callback is None:\n            # Return a decorator function that will receive the actual callback\n            def decorator(func):\n                if event not in self._events:\n                    self._events[event] = []\n                self._events[event].append(func)\n                return func\n            return decorator\n        else:\n            # Regular method usage\n            if event not in self._events:\n                self._events[event] = []\n            self._events[event].append(callback)\n\n    def emit(self, event, *args, **kwargs):\n        if event in self._events:\n            for callback in self._events[event]:\n                callback(*args, **kwargs)\n\nemitter = EventEmitter()\n\n# Define a decorator to register an event callback\n", "suffix": "def on_user_logged_in(user):\n    print(f\"User {user} has logged in.\")\n\nemitter.emit('user_logged_in', 'Alice')\n\n# Assertions\nassert 'user_logged_in' in emitter._events\nassert len(emitter._events['user_logged_in']) == 1\nassert callable(emitter._events['user_logged_in'][0])\n", "golden_completion": "@emitter.on('user_logged_in')\n", "LLM_justification": "This test case is a good example of testing low-context pattern matching capabilities because it involves recognizing a common decorator pattern used to register event callbacks. The prefix provides enough context to understand that a decorator is needed to register an event callback, but the completion itself requires understanding the specific event and the decorator syntax. The suffix demonstrates the event being emitted and includes assertions to verify the correct behavior, making it clear and distinct from the golden completion.", "assertions": "emitter.emit('user_logged_in', 'Alice')\nassert 'user_logged_in' in emitter._events\nassert len(emitter._events['user_logged_in']) == 1\nassert callable(emitter._events['user_logged_in'][0])\n", "gpt-4.1-mini": "@emitter.on('user_logged_in')"}
{"id": "12", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\nimport io\nimport sys\n\nclass EventSystem:\n    def __init__(self):\n        self._events = {}\n\n    def on(self, event_name, callback):\n        if event_name not in self._events:\n            self._events[event_name] = []\n        self._events[event_name].append(callback)\n\n    def trigger(self, event_name, *args, **kwargs):\n        if event_name in self._events:\n            for callback in self._events[event_name]:\n                callback(*args, **kwargs)\n\n    def clear_event(self, event_name):\n        if event_name in self._events:\n            del self._events[event_name]\n\n# Example callback functions\ndef on_data_received(data):\n    print(f\"Data received: {data}\")\n\ndef on_error_occurred(error):\n    print(f\"Error: {error}\")\n\n# Create an event system instance\nevent_system = EventSystem()\n\n# Register events\n", "suffix": "", "golden_completion": "event_system.on('data', on_data_received)\nevent_system.on('error', on_error_occurred)\n\n", "LLM_justification": "This test case evaluates the model's ability to recognize and complete an event handling pattern. The prefix sets up an event system with registration and triggering of events, providing context for how callbacks should be registered and triggered. The golden completion correctly adds event listeners and triggers them, while the suffix tests the triggering and clearing of events. The scenario is realistic and tests the model's ability to continue established patterns in event handling code.", "assertions": "import io\nimport sys\n\n# Redirect stdout to capture print outputs\ncaptured_output = io.StringIO()\nsys.stdout = captured_output\n\n# Trigger events\ntry:\n    event_system.trigger('data', 'Hello, World!')\n    event_system.trigger('error', '404 Not Found')\nexcept Exception as e:\n    print(f\"Exception occurred: {e}\")\nfinally:\n    # Reset redirect.\n    sys.stdout = sys.__stdout__\n\n# Verify the captured output\noutput = captured_output.getvalue().strip().split('\\n')\nassert output[0] == 'Data received: Hello, World!'\nassert output[1] == 'Error: 404 Not Found'\nassert len(output) == 2\n", "gpt-4.1-mini": "event_system.on('data_received', on_data_received)\nevent_system.on('error_occurred', on_error_occurred)"}
{"id": "13", "testsource": "synthbench-low-context", "language": "python", "prefix": "import time\nimport threading\n\nclass EventListener:\n    def __init__(self):\n        self.events = []\n\n    def on_event(self, callback):\n        self.events.append(callback)\n\n    def trigger_event(self, event_data):\n        for callback in self.events:\n            callback(event_data)\n\nclass EventSource:\n    def __init__(self):\n        self.listener = EventListener()\n\n    def start(self):\n        def run():\n            for i in range(5):\n                time.sleep(1)\n                self.listener.trigger_event(f'Event {i}')\n        threading.Thread(target=run).start()\n\nsource = EventSource()\n\n# Register a callback to handle events\nsource.listener.on_event(lambda data: print(f'Received: {data}'))\n\nsource.start()\n", "suffix": "source.listener.on_event(lambda data: print(f'Handled: {data}'))\n\n# Ensuring all events are processed\nimport time\n\n# Wait for the events to be processed\nfor _ in range(6):\n    time.sleep(1)\n\nassert 'Event 0' in captured_output\nassert 'Event 1' in captured_output\nassert 'Event 2' in captured_output\nassert 'Event 3' in captured_output\nassert 'Event 4' in captured_output\n\n# Clean up any resources if necessary\n# (None in this example as threading automatically cleans up)\n", "golden_completion": "captured_output = []\nsource.listener.on_event(lambda data: captured_output.append(data))\n", "LLM_justification": "This test case is a good example of callback and event handling patterns. It sets up an event listener and an event source, then triggers events and verifies that callbacks are executed correctly. The completion involves capturing output using a list, which is a common pattern in handling callbacks. Multiple valid completions may exist, such as different ways to capture and verify the events.", "assertions": "assert 'Event 0' in captured_output\nassert 'Event 1' in captured_output\nassert 'Event 2' in captured_output\nassert 'Event 3' in captured_output\nassert 'Event 4' in captured_output\n", "gpt-4.1-mini": "captured_output = []\n\ndef capture(data):\n    print(f'Handled: {data}')\n    captured_output.append(data)\n\nsource.listener.on_event(capture)"}
{"id": "14", "testsource": "synthbench-low-context", "language": "python", "prefix": "import sqlite3\n\nclass Database:\n    def __init__(self, db_name):\n        self.connection = sqlite3.connect(db_name)\n        self.cursor = self.connection.cursor()\n\n    def create_table(self):\n        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS users (id INTEGER PRIMARY KEY, name TEXT)\")\n\n    def insert_user(self, name):\n        self.cursor.execute(\"INSERT INTO users (name) VALUES (?)\", (name,))\n        self.connection.commit()\n\n    def fetch_users(self):\n        self.cursor.execute(\"SELECT * FROM users\")\n        return self.cursor.fetchall()\n\n    def close(self):\n        self.connection.close()\n\n# Create an instance of the Database class\n", "suffix": "\n# Fetch users and validate the insert\nusers = db.fetch_users()\nassert len(users) == 1\nassert users[0][1] == 'John Doe'\n\n", "golden_completion": "db = Database(':memory:')\ndb.create_table()\ndb.insert_user('John Doe')", "LLM_justification": "This test case is a good example of a common usage pattern involving database operations. It includes creating a table, inserting a record, and fetching the record, which are frequent tasks in many applications. The LLM needs to recognize the pattern of creating an instance, performing operations, and ensuring cleanup. This example tests the LLM's ability to follow a standard sequence of database operations and handle minimal context effectively.", "assertions": "assert db.fetch_users() == [(1, 'John Doe')]\n\n# Close the database connection\ndb.close()\n", "gpt-4.1-mini": "db = Database('test.db')\ndb.create_table()\ndb.insert_user('John Doe')"}
{"id": "15", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\n\nclass Event:\n    def __init__(self):\n        self._callbacks = []\n\n    def register(self, callback):\n        self._callbacks.append(callback)\n\n    def emit(self, *args, **kwargs):\n        for callback in self._callbacks:\n            callback(*args, **kwargs)\n\nclass Button:\n    def __init__(self):\n        self.click_event = Event()\n\n    def click(self):\n        self.click_event.emit()\n\nbutton = Button()\n\n# Registering a callback\n", "suffix": "# Another action on click\n@functools.lru_cache(maxsize=None)\ndef cached_click_action():\n    print(\"Cached click action executed\")\n\nbutton.click_event.register(cached_click_action)\n\n# Simulating button clicks\nbutton.click()\nbutton.click()\n\nassert len(button.click_event._callbacks) == 2, \"There should be two callbacks registered\"\nassert not button.click_event._callbacks[1] is button.click_event._callbacks[0], \"Callbacks should be different instances\"\n", "golden_completion": "button.click_event.register(lambda: print(\"Button clicked!\"))\n", "LLM_justification": "This is a good test case for callback and event handling patterns because it demonstrates how to register and emit events with different kinds of callbacks. The prefix sets up a clear event system and the suffix shows another way to register a callback, testing the LLM's ability to recognize and continue the pattern by registering a simple lambda function as a callback. This tests the LLM's understanding of event-driven programming and callback registration.", "assertions": "button.click()\nbutton.click()\n\nassert len(button.click_event._callbacks) == 2, \"There should be two callbacks registered\"\nassert not button.click_event._callbacks[1] is button.click_event._callbacks[0], \"Callbacks should be different instances\"\n", "gpt-4.1-mini": "def click_action():\n    print(\"Button clicked\")\n\nbutton.click_event.register(click_action)"}
{"id": "16", "testsource": "synthbench-low-context", "language": "python", "prefix": "def read_file(file_path):\n    try:\n        with open(file_path, 'r') as file:\n            content = file.read()\n    except FileNotFoundError:\n        return None\n", "suffix": "\n\n# Test the function\nfile_path = 'test_file.txt'\nwith open(file_path, 'w') as f:\n    f.write('Hello, world!')\n\nresult = read_file(file_path)\nassert result == 'Hello, world!'\nassert read_file('non_existent_file.txt') is None\n", "golden_completion": "    return content", "LLM_justification": "This test case focuses on a universal pattern of reading a file, handling exceptions, and performing cleanup. It tests the model's ability to complete the function correctly by returning the content read from the file. The scenario includes proper exception handling for a common file operation and ensures cleanup after assertions.", "assertions": "assert result == 'Hello, world!'\nassert read_file('non_existent_file.txt') is None", "gpt-4.1-mini": "    return content"}
{"id": "17", "testsource": "synthbench-low-context", "language": "python", "prefix": "import os\n\ndef create_temp_file():\n    temp_file = 'temp.txt'\n    with open(temp_file, 'w') as f:\n        f.write('Temporary data')\n    return temp_file\n\ntemp_file = create_temp_file()\n\ntry:\n    # Read the file content", "suffix": "\nfinally:\n    if os.path.exists(temp_file):\n        os.remove(temp_file)\n        print(f\"Cleaned up {temp_file}\")", "golden_completion": "    with open(temp_file, 'r') as f:\n        content = f.read()", "LLM_justification": "This example tests the model's ability to recognize the standard pattern of working with temporary files in Python. The completion should demonstrate the ability to open and read from a file in a context where the file is created and cleaned up properly. This pattern is widely used and follows well-established conventions.", "assertions": "assert content == 'Temporary data'\nassert os.path.exists(temp_file) == False", "gpt-4.1-mini": "    with open(temp_file, 'r') as f:\n        content = f.read()\n    print(content)"}
{"id": "18", "testsource": "synthbench-low-context", "language": "python", "prefix": "import os\n\ndef create_and_write_file(file_path, content, auto_delete=True):\n    try:\n        # Create the file and write content to it\n", "suffix": "    finally:\n        # Clean up: delete the file if auto_delete is True\n        if auto_delete and os.path.exists(file_path):\n            os.remove(file_path)\n\n# Test the function\nfile_path = 'testfile.txt'\ncontent = 'Hello, World!'\ncreate_and_write_file(file_path, content)\n", "golden_completion": "        with open(file_path, 'w') as file:\n            file.write(content)", "LLM_justification": "This is a good test case because it involves a common file operation pattern: creating and writing to a file within a context manager to ensure proper handling of file resources. The context manager pattern is widely used and recognized in Python, making it a universal pattern. Additionally, it tests the LLM's ability to handle resource cleanup correctly with minimal context provided.", "assertions": "assert not os.path.exists(file_path), 'File should be deleted after function execution'\ntry:\n    create_and_write_file(file_path, content, auto_delete=False)\n    with open(file_path, 'r') as file:\n        assert file.read() == content, 'File content should match the written content'\nfinally:\n    if os.path.exists(file_path):\n        os.remove(file_path)\n", "gpt-4.1-mini": "        with open(file_path, 'w') as f:\n            f.write(content)"}
{"id": "19", "testsource": "synthbench-low-context", "language": "python", "prefix": "import os\nimport tempfile\n\nclass FileManager:\n    def __init__(self, filename):\n        self.filename = filename\n\n    def write_data(self, data):\n", "suffix": "\n    def read_data(self):\n        with open(self.filename, 'r') as f:\n            return f.read()\n\n# Create a temporary file\nwith tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n    file_path = tmp_file.name\n\n# Initialize FileManager\nmanager = FileManager(file_path)\n\n# Write data to file\nmanager.write_data('Sample Data')\n\n# Read data from file\nassert manager.read_data() == 'Sample Data'\n\n", "golden_completion": "        with open(self.filename, 'w') as f:\n            f.write(data)\n", "LLM_justification": "This scenario tests the model's ability to recognize a common pattern in file operations where data is written to a file. The pattern of opening a file, writing data to it, and closing the file is widely used and should be recognizable with minimal context. The completion demonstrates understanding of how to correctly open a file in write mode and write data to it, which is a universal programming pattern.", "assertions": "assert os.path.exists(manager.filename)\nmanager.write_data('Test Data')\nassert manager.read_data() == 'Test Data'\n\n# Cleanup\nos.remove(file_path)\n", "gpt-4.1-mini": "        with open(self.filename, 'w') as f:\n            f.write(data)"}
{"id": "20", "testsource": "synthbench-low-context", "language": "python", "prefix": "import json\n\ndef save_to_file(data, filename):\n    with open(filename, 'w') as file:\n", "suffix": "", "golden_completion": "        json.dump(data, file)\n", "LLM_justification": "This test case examines the ability to recognize a standard file operation pattern in Python with minimal context. It focuses on using the json module to serialize data and write it to a file within a 'with' statement context. The pattern of using 'json.dump' for writing JSON data to a file is common, and the minimal context should be sufficient for recognizing this standard operation. The provided golden completion aligns with best practices for writing JSON data to a file.", "assertions": "data = {'key': 'value'}\nfilename = 'testfile.json'\nsave_to_file(data, filename)\n\nwith open(filename, 'r') as file:\n    content = file.read()\n    assert content == '{\"key\": \"value\"}'\n\n# Cleanup\nimport os\nos.remove(filename)\n", "gpt-4.1-mini": "        json.dump(data, file)"}
{"id": "21", "testsource": "synthbench-low-context", "language": "python", "prefix": "import json\n\ndef load_json(file_path):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n        return data\n\n", "suffix": "\n\n# Prepare test data\nfile_path = 'test.json'\nsave_json(file_path, {'name': 'example'})\ndata = load_json(file_path)\n\n# Assertions to verify correctness\nassert isinstance(data, dict)\nassert 'name' in data\nassert data['name'] == 'example'\n\n", "golden_completion": "def save_json(file_path, data):\n    with open(file_path, 'w') as file:\n        json.dump(data, file)", "LLM_justification": "This example tests the model's ability to recognize a common file I/O pattern, specifically loading and saving JSON data. The task is to complete the function that saves JSON data to a file, following the pattern established by the load_json function. This pattern is universal and follows well-established conventions. The assertions verify that the data is correctly saved and loaded, ensuring the completion maintains consistency with the prefix and suffix.", "assertions": "assert isinstance(data, dict)\nassert 'name' in data\nassert data['name'] == 'example'\n\n# Cleanup\nimport os\nos.remove(file_path)", "gpt-4.1-mini": "def save_json(file_path, data):\n    with open(file_path, 'w') as file:\n        json.dump(data, file)"}
{"id": "22", "testsource": "synthbench-low-context", "language": "python", "prefix": "import os\nimport tempfile\n\nclass FileManager:\n    def __init__(self):\n        self.temp_dir = tempfile.TemporaryDirectory()\n\n    def create_temp_file(self, filename, content):\n        file_path = os.path.join(self.temp_dir.name, filename)\n        with open(file_path, 'w') as f:\n            f.write(content)\n        return file_path\n\n    def read_temp_file(self, filename):\n        file_path = os.path.join(self.temp_dir.name, filename)\n        with open(file_path, 'r') as f:\n            return f.read()\n\n    def delete_temp_file(self, filename):\n        file_path = os.path.join(self.temp_dir.name, filename)\n        if os.path.exists(file_path):\n", "suffix": "\n\n    def __del__(self):\n        self.temp_dir.cleanup()\n\n# Example usage\nfm = FileManager()\nfile_path = fm.create_temp_file('test.txt', 'Hello, World!')\ncontent = fm.read_temp_file('test.txt')\nfm.delete_temp_file('test.txt')\n\nassert content == 'Hello, World!'\nassert not os.path.exists(file_path)", "golden_completion": "            os.remove(file_path)", "LLM_justification": "This is a good test case because it demonstrates a universal pattern of file operations within a temporary directory, which is a common scenario in Python programming. The `delete_temp_file` method is a standard way to remove files, and the pattern of checking for file existence before removing it is widely recognized. The prompt establishes a clear context for the operations, and the completion follows well-established conventions, ensuring semantic clarity. The assertions verify the correctness of file creation, reading, and deletion, ensuring the model's understanding of the pattern.", "assertions": "\nassert not os.path.exists(file_path)", "gpt-4.1-mini": "            os.remove(file_path)"}
{"id": "23", "testsource": "synthbench-low-context", "language": "python", "prefix": "import json\n\ndef save_to_file(data, filename):\n    with open(filename, 'w') as file:\n        json.dump(data, file)\n\ndef load_from_file(filename):\n    with open(filename, 'r') as file:\n", "suffix": "        return data\n\n# Sample usage\ndata = {'key': 'value'}\nfilename = 'testfile.json'\nsave_to_file(data, filename)\nloaded_data = load_from_file(filename)\nassert loaded_data == data\n\n", "golden_completion": "        data = json.load(file)\n", "LLM_justification": "This test case is a good example of low-context pattern matching because it relies on the model's ability to recognize the standard pattern of using the `json` module to read and write data to files in Python. The pattern of `json.load` and `json.dump` is a universal, well-established convention in Python programming. The prompt and suffix provide enough context to suggest the pattern, and the completion must correctly follow this pattern to pass the assertions.", "assertions": "assert load_from_file('testfile.json') == {'key': 'value'}\n", "gpt-4.1-mini": "        data = json.load(file)"}
{"id": "24", "testsource": "synthbench-low-context", "language": "python", "prefix": "import os\n\ndef read_file_content(file_path):\n    try:", "suffix": "\n    except FileNotFoundError:\n        return None\n    except Exception as e:\n        raise e\n\n# Test cases\nif __name__ == '__main__':\n    # Create a sample file for testing\n    test_file_path = 'test_file.txt'\n    with open(test_file_path, 'w') as f:\n        f.write('Sample content')\n\n    # Assertions\n    assert read_file_content(test_file_path) == 'Sample content'\n    assert read_file_content('nonexistent_file.txt') is None\n\n", "golden_completion": "        with open(file_path, 'r') as file:\n            return file.read()", "LLM_justification": "This test case is designed to evaluate the model's ability to recognize and complete a common pattern for reading file content in Python. The context is minimal, yet the pattern of opening a file and reading its content is a widely-used and standardized practice. The completion must handle the file correctly and ensure that resources are properly managed. The assertions verify that the function behaves as expected with both existing and non-existing files.", "assertions": "assert read_file_content(test_file_path) == 'Sample content'\nassert read_file_content('nonexistent_file.txt') is None", "gpt-4.1-mini": "        with open(file_path, 'r') as file:\n            return file.read()"}
{"id": "25", "testsource": "synthbench-low-context", "language": "python", "prefix": "import os\n\ndef read_file(file_path):\n    try:\n        with open(file_path, 'r') as file:\n            data = file.read()\n", "suffix": "    except Exception as e:\n        print(f\"Error reading file: {e}\")\n\n# Test case\nfile_path = 'test.txt'\nwith open(file_path, 'w') as f:\n    f.write('Hello, World!')\n\nresult = read_file(file_path)\nos.remove(file_path)\n", "golden_completion": "            return data\n", "LLM_justification": "This example tests the model's ability to recognize and complete a common file reading pattern using Python's `with` statement for resource management. The code involves reading a file and handling potential exceptions, which is a universal pattern in Python programming. The prompt and suffix provide enough context to understand the typical use of `with open`, error handling, and returning the read data. The golden completion maintains this pattern and ensures proper cleanup and error handling, making this a good test case for low-context pattern matching.", "assertions": "assert result == 'Hello, World!'\n", "gpt-4.1-mini": "            return data"}
{"id": "26", "testsource": "synthbench-low-context", "language": "python", "prefix": "import os\n\nclass FileHandler:\n    def __init__(self, filepath):\n        self.filepath = filepath\n\n    def read_file(self):\n        try:\n", "suffix": "            return content\n        finally:\n            file.close()\n\n# Testing the FileHandler class\nhandler = FileHandler('testfile.txt')\nwith open('testfile.txt', 'w') as f:\n    f.write('Hello, World!')\n\nresult = handler.read_file()\nassert result == 'Hello, World!'\n\n", "golden_completion": "            file = open(self.filepath, 'r')\n            content = file.read()\n", "LLM_justification": "This test case involves a common file handling pattern where a file is opened, read, and then closed using a try/finally block. This pattern is widely used and follows standard conventions, making it a good candidate for testing low-context pattern recognition. The completion demonstrates the correct usage of opening a file and reading its content, while the assertions validate the behavior, ensuring the file content is read correctly. The cleanup code ensures no resources are left open or files left behind, maintaining good practice.", "assertions": "assert handler.read_file() == 'Hello, World!'\n", "gpt-4.1-mini": "            file = open(self.filepath, 'r')\n            content = file.read()"}
{"id": "27", "testsource": "synthbench-low-context", "language": "python", "prefix": "import json\n\nclass ConfigManager:\n    def __init__(self, config_file):\n        self.config_file = config_file\n        self.config = None\n\n    def load_config(self):\n        with open(self.config_file, 'r') as file:", "suffix": "\n\n    def get_config_value(self, key):\n        return self.config.get(key, None)\n", "golden_completion": "\n            self.config = json.load(file)", "LLM_justification": "This scenario tests the LLM's ability to recognize the common pattern of reading a JSON configuration file in Python. The prefix provides minimal context, requiring the LLM to infer the necessary completion based on the usage of the json module and file handling. The pattern of loading a JSON file is universal and follows a well-established convention, making it ideal for testing low-context pattern recognition.", "assertions": "import json\nwith open('test_config.json', 'w') as file:\n    json.dump({'key1': 'value1', 'key2': 42}, file)\n\nconfig_manager = ConfigManager('test_config.json')\nconfig_manager.load_config()\n\nassert config_manager.get_config_value('key1') == 'value1'\nassert config_manager.get_config_value('key2') == 42\n\n\nimport os\nos.remove('test_config.json')", "gpt-4.1-mini": "            self.config = json.load(file)"}
{"id": "28", "testsource": "synthbench-low-context", "language": "python", "prefix": "import os\nimport tempfile\n\ndef create_temp_file(content):\n    temp_file = tempfile.NamedTemporaryFile(delete=False)\n    try:\n        temp_file.write(content.encode())\n        temp_file.close()\n        return temp_file.name\n    except Exception as e:\n        temp_file.close()\n        os.remove(temp_file.name)\n        raise e\n\n# Creating a temporary file with some content\nfile_path = create_temp_file('Hello, World!')\n# Read and assert\n", "suffix": "\n# Cleanup\nif os.path.exists(file_path):\n    os.remove(file_path)", "golden_completion": "with open(file_path, 'r') as file:\n    content = file.read()\n\nassert content == 'Hello, World!'", "LLM_justification": "This example tests the model's ability to recognize and complete a common pattern for file operations in Python, specifically reading from a file. The pattern of opening a file with 'with' statement and reading its content is a widely-used and standardized practice. The prompt sets up the context of creating a temporary file, and the completion should naturally follow with reading from that file. The assertions ensure that the file content is as expected, verifying that the pattern was correctly followed.", "assertions": "assert content == 'Hello, World!'", "gpt-4.1-mini": "with open(file_path, 'r') as f:\n    data = f.read()\nassert data == 'Hello, World!'"}
{"id": "29", "testsource": "synthbench-low-context", "language": "python", "prefix": "import os\n\ndef create_and_write_file(filename, content):\n    try:\n        with open(filename, 'w') as f:\n", "suffix": "        os.remove(filename)\n    except OSError as e:\n        print(f\"Error: {e}\")\n\n# Test the function\nfilename = 'testfile.txt'\ncontent = 'Hello, world!'\ncreate_and_write_file(filename, content)\n", "golden_completion": "            f.write(content)\n", "LLM_justification": "This is a good test case because it checks for the ability to recognize a common pattern of writing to a file with minimal context. The pattern of opening a file, writing content, and handling exceptions is universally used and follows well-established conventions in Python. The provided context is minimal but sufficient to identify the need to write to the file. The suffix includes cleanup code and exception handling, ensuring the resource is cleaned up after assertions.", "assertions": "assert os.path.exists(filename) == False\n", "gpt-4.1-mini": "            f.write(content)"}
{"id": "30", "testsource": "synthbench-low-context", "language": "python", "prefix": "class FileManager:\n    def __init__(self, file_name):\n        self.file_name = file_name\n        self.file = None\n\n    def __enter__(self):\n        self.file = open(self.file_name, 'r')\n        return self.file\n\n", "suffix": "", "golden_completion": "    def __exit__(self, exc_type, exc_value, traceback):\n        if self.file:\n            self.file.close()\n", "LLM_justification": "This test case is a good example of a context manager and resource handling pattern in Python. The pattern is clear and identifiable with minimal context and follows widely-used conventions for managing file resources using the `with` statement. The completion demonstrates proper resource cleanup and error handling. The assertions validate that the file content is read correctly, and the cleanup code ensures the file is removed after the test.", "assertions": "with open('test.txt', 'w') as f:\n    f.write('Hello, world!')\n\nwith FileManager('test.txt') as f:\n    content = f.read()\n\nassert content == 'Hello, world!'\n", "gpt-4.1-mini": "    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.file:\n            self.file.close()"}
{"id": "31", "testsource": "synthbench-low-context", "language": "python", "prefix": "class ResourceHandler:\n    def __init__(self, resource_name):\n        self.resource_name = resource_name\n        self.resource = None\n\n    def __enter__(self):\n        self.resource = open(self.resource_name, 'w')\n        return self.resource\n\n", "suffix": "\n\nwith ResourceHandler('testfile.txt') as f:\n    f.write('Hello, World!')\n\n# Clean up\nimport os\nos.remove('testfile.txt')\n", "golden_completion": "    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.resource:\n            self.resource.close()\n        if exc_type:\n            raise", "LLM_justification": "This is a good test case because it involves context managers, which are a common resource handling pattern in Python. The pattern is recognizable with minimal context, requiring the completion to correctly close the resource and handle exceptions.", "assertions": "with ResourceHandler('testfile.txt') as f:\n    f.write('Hello, World!')\nassert os.path.exists('testfile.txt')\nwith open('testfile.txt', 'r') as f:\n    assert f.read() == 'Hello, World!'\n# Clean up\nos.remove('testfile.txt')", "gpt-4.1-mini": "    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.resource:\n            self.resource.close()"}
{"id": "32", "testsource": "synthbench-low-context", "language": "python", "prefix": "def process_text(text):\n    # Convert the text to uppercase\n    upper_text = text.upper()\n    # Replace spaces with underscores\n", "suffix": "\n    return transformed_text\n\n# Test cases\nassert process_text('hello world') == 'HELLO_WORLD'\nassert process_text('Python Programming') == 'PYTHON_PROGRAMMING'\nassert process_text('Test Case') == 'TEST_CASE'", "golden_completion": "    transformed_text = upper_text.replace(' ', '_')", "LLM_justification": "This is a good test case because it involves a common string manipulation pattern where text is transformed by converting to uppercase and replacing spaces with underscores. The pattern is recognizable with minimal context and follows widely-used conventions. The completion demonstrates understanding of chaining string methods and maintaining the transformation order, which is essential for correct functionality.", "assertions": "assert process_text('hello world') == 'HELLO_WORLD'\nassert process_text('Python Programming') == 'PYTHON_PROGRAMMING'\nassert process_text('Test Case') == 'TEST_CASE'", "gpt-4.1-mini": "    transformed_text = upper_text.replace(' ', '_')"}
{"id": "33", "testsource": "synthbench-low-context", "language": "python", "prefix": "class FileManager:\n    def __init__(self, filename):\n        self.filename = filename\n\n    def __enter__(self):\n        self.file = open(self.filename, 'r')\n        return self.file\n", "suffix": "\n\nwith open('example.txt', 'w') as f:\n    f.write('This is a test file.')\n\nwith FileManager('example.txt') as f:\n    content = f.read()\n\n", "golden_completion": "    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.file.close()\n", "LLM_justification": "This is a good test case because it checks the model's ability to recognize and complete the context manager pattern in Python. The pattern of implementing `__enter__` and `__exit__` methods is widely used and recognizable with minimal context. The completion must ensure that resources are properly managed, which is critical in real-world applications. The assertions verify that the file was correctly opened and read, and the resource management was handled as expected.", "assertions": "assert content == 'This is a test file.'\n\nimport os\nos.remove('example.txt')\n", "gpt-4.1-mini": "    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.file.close()"}
{"id": "34", "testsource": "synthbench-low-context", "language": "python", "prefix": "class MyClass:\n    def __init__(self, value):\n        self.value = value\n\n    def increment(self, amount):\n        self.value += amount\n\n    def display_value(self):\n        print(f'Value: {self.value}')", "suffix": "\n\n# Creating an instance of MyClass\nobj = MyClass(10)\n# Incrementing the value\nobj.increment(5)\n# Displaying the current value\nobj.display_value()\n\n# Resetting the value to 0\nobj.reset_value()\n\n# Displaying the reset value\nobj.display_value()", "golden_completion": "\n\n    def reset_value(self):\n        self.value = 0\n\n", "LLM_justification": "This test case evaluates the LLM's ability to recognize and continue an object-oriented pattern by adding a new method to a class. The completion must maintain consistency with the class's method definitions and should properly manipulate the class attribute. The pattern of defining and adding methods to a class is a common practice in OOP, and the LLM should be able to handle it with minimal context.", "assertions": "obj.reset_value()\nassert obj.value == 0\nobj.increment(10)\nassert obj.value == 10\nobj.reset_value()\nassert obj.value == 0\n\n# Reset value test\nobj.value = 15\nobj.reset_value()\nassert obj.value == 0\n", "gpt-4.1-mini": "    def reset_value(self):\n        self.value = 0"}
{"id": "35", "testsource": "synthbench-low-context", "language": "python", "prefix": "import threading\n\nclass Counter:\n    def __init__(self):\n        self.count = 0\n        self.lock = threading.Lock()\n\n    def increment(self):\n        with self.lock:\n", "suffix": "\n        return self.count\n\ncounter = Counter()\nthreads = []\nfor _ in range(10):\n    t = threading.Thread(target=counter.increment)\n    threads.append(t)\n    t.start()\nfor t in threads:\n    t.join()\n\nassert counter.count == 10, f\"Expected count to be 10, but got {counter.count}\"\n", "golden_completion": "            self.count += 1", "LLM_justification": "This example tests the LLM's ability to recognize and complete the pattern of thread-safe data structure manipulation using context managers and resource handling. The prompt establishes a clear pattern of using a lock within a context manager to safely increment a counter, and the golden completion requires the LLM to understand and follow this pattern. The suffix includes assertions to verify the correctness of the implementation and ensure the counter is incremented correctly in a multithreaded environment.", "assertions": "assert counter.count == 10, f\"Expected count to be 10, but got {counter.count}\"", "gpt-4.1-mini": "            self.count += 1"}
{"id": "36", "testsource": "synthbench-low-context", "language": "python", "prefix": "class FileHandler:\n    def __init__(self, filename):\n        self.filename = filename\n\n    def __enter__(self):\n        self.file = open(self.filename, 'r')\n        return self.file\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n", "suffix": "\n\n# Usage example\ndef read_first_line(filename):\n    with FileHandler(filename) as f:\n        return f.readline()\n\n", "golden_completion": "        if self.file:\n            self.file.close()\n        return False\n        \n# Alternatively:\n#        if exc_type:\n#            return False\n#        return True", "LLM_justification": "This example tests the LLM's ability to recognize and complete a context manager pattern in Python, focusing on proper resource handling and cleanup in the __exit__ method. The completion must correctly handle the file resource and return appropriate values, demonstrating an understanding of the context manager protocol with minimal surrounding context.", "assertions": "import os\nwith open('testfile.txt', 'w') as f:\n    f.write('First line of the file\\nSecond line\\n')\nassert read_first_line('testfile.txt') == 'First line of the file\\n'\nos.remove('testfile.txt')", "gpt-4.1-mini": "        self.file.close()"}
{"id": "37", "testsource": "synthbench-low-context", "language": "python", "prefix": "class MyClass:\n    def __init__(self, value):\n        self.value = value\n\n    def add(self, x):\n        return self.value + x\n\n    def multiply(self, x):\n        return self.value * x\n\n    def subtract(self, x):", "suffix": "\n\n# Test cases\nobj = MyClass(10)\nassert obj.add(5) == 15\nassert obj.multiply(2) == 20\nassert obj.subtract(3) == 7\n\nprint(\"All tests passed!\")", "golden_completion": "        return self.value - x", "LLM_justification": "This is a good test case because it requires the LLM to recognize the pattern of class method definitions and correctly complete the 'subtract' method based on the context provided by the 'add' and 'multiply' methods. The pattern involves performing arithmetic operations on 'self.value' with the parameter 'x'. The completion demonstrates understanding of this pattern by implementing the 'subtract' method to return the result of subtracting 'x' from 'self.value'.", "assertions": "obj = MyClass(10)\nassert obj.add(5) == 15\nassert obj.multiply(2) == 20\nassert obj.subtract(3) == 7", "gpt-4.1-mini": "        return self.value - x"}
{"id": "38", "testsource": "synthbench-low-context", "language": "python", "prefix": "class DataProcessor:\n    def __init__(self, data):\n        self.data = data\n\n    def filter_data(self, threshold):\n        '''Filters data elements greater than the threshold'''\n", "suffix": "\n    def process_data(self):\n        '''Processes data by filtering and then sorting it'''\n        self.filtered_data = self.filter_data(10)\n        self.sorted_data = sorted(self.filtered_data)\n\n# Test the DataProcessor class\nprocessor = DataProcessor([5, 20, 15, 10, 25, 7])\nprocessor.process_data()\n\n# Assertions\nassert processor.filtered_data == [20, 15, 25]\nassert processor.sorted_data == [15, 20, 25]\n", "golden_completion": "        return [x for x in self.data if x > threshold]", "LLM_justification": "This test case is good because it evaluates the LLM's ability to recognize and continue a pattern of filtering data within a class method. The prompt provides minimal context but establishes a clear pattern of data processing within class methods. The golden completion involves filtering data based on a threshold, a common and universal programming task, and the assertions validate the correct functionality of the implemented method.", "assertions": "assert DataProcessor([5, 20, 15, 10, 25, 7]).filter_data(10) == [20, 15, 25]\nassert DataProcessor([1, 2, 3, 4, 5]).filter_data(3) == [4, 5]\n", "gpt-4.1-mini": "        return [x for x in self.data if x > threshold]"}
{"id": "39", "testsource": "synthbench-low-context", "language": "python", "prefix": "class StringProcessor:\n    def __init__(self, text):\n        self.text = text\n\n    def process_text(self):\n        # Convert text to uppercase\n        self.text = self.text.upper()\n\n    def extract_vowels(self):\n        vowels = 'AEIOU'\n        return ''.join([char for char in self.text if char in vowels])\n\n    def __str__(self):\n        return self.text\n\n# Example usage:\nprocessor = StringProcessor(\"hello world\")\nprocessor.process_text()\n# Extract vowels\n", "suffix": "\nassert processor.text == \"HELLO WORLD\"\nvowels = processor.extract_vowels()\nassert vowels == \"EOO\"\n\n# Clean up\n", "golden_completion": "vowels = processor.extract_vowels()\nassert vowels == \"EOO\"\n", "LLM_justification": "This test case evaluates the LLM's ability to recognize and complete a pattern involving object-oriented programming, specifically class methods and string manipulation. The example establishes a clear context for class instantiation, method calling, and string processing. The LLM must infer the logical next step to call the `extract_vowels` method and verify the result, demonstrating an understanding of class method usage and return value handling.", "assertions": "assert processor.text == \"HELLO WORLD\"\nvowels = processor.extract_vowels()\nassert vowels == \"EOO\"\n", "gpt-4.1-mini": "vowels = processor.extract_vowels()"}
{"id": "40", "testsource": "synthbench-low-context", "language": "python", "prefix": "class MyClass:\n    def __init__(self, value):\n        self.value = value\n\n    def double_value(self):\n", "suffix": "\n\n# Testing the MyClass\na = MyClass(10)\nassert a.double_value() == 20\na.value = 5\nassert a.double_value() == 10\n", "golden_completion": "        return self.value * 2", "LLM_justification": "This example tests the LLM's ability to recognize and complete an object-oriented pattern involving a class method. The pattern is identifiable with minimal context and is universally understood. The method 'double_value' should return twice the value of 'self.value'. This tests the model's understanding of basic class method implementation.", "assertions": "a.value = 10\nassert a.double_value() == 20\na.value = 5\nassert a.double_value() == 10", "gpt-4.1-mini": "        return self.value * 2"}
{"id": "41", "testsource": "synthbench-low-context", "language": "python", "prefix": "class Animal:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        raise NotImplementedError('Subclasses must implement this method')\n\nclass Dog(Animal):\n", "suffix": "\ndog = Dog('Buddy')\nassert dog.speak() == 'Woof, I am Buddy'\n", "golden_completion": "    def speak(self):\n        return f'Woof, I am {self.name}'", "LLM_justification": "This example tests the model's ability to recognize and complete an object-oriented pattern involving class inheritance and method overriding. The prompt establishes a base class with an abstract method, and the suffix sets up an assertion that requires the subclass to implement the method correctly. The pattern of defining a method in a subclass to override a base class method is common and should be recognizable with minimal context.", "assertions": "dog = Dog('Buddy')\nassert dog.speak() == 'Woof, I am Buddy'\n", "gpt-4.1-mini": "    def speak(self):\n        return f'Woof, I am {self.name}'"}
{"id": "42", "testsource": "synthbench-low-context", "language": "python", "prefix": "class DataProcessor:\n    def __init__(self, data):\n        self.data = data\n\n    def process_data(self):\n        result = []\n        for item in self.data:\n            if self.is_valid(item):\n                result.append(self.transform(item))\n        return result\n\n    def is_valid(self, item):\n        # Custom validation logic\n", "suffix": "\n    def transform(self, item):\n        # Custom transformation logic\n        return item.upper()\n\n# Example usage\nprocessor = DataProcessor(['a', 'b', 'c', '1', '2'])\nprocessed_data = processor.process_data()\n\nassert processed_data == ['A', 'B', 'C']\n", "golden_completion": "        return item.isalpha()", "LLM_justification": "This test case evaluates the LLM's ability to recognize and complete a pattern within an object-oriented context. The 'is_valid' method needs to implement a validation check that aligns with the given data processing pattern. The prompt and suffix provide enough context to understand that 'is_valid' should filter alphabetic characters only, which is a common string validation check.", "assertions": "processor = DataProcessor(['a', 'b', 'c', '1', '2'])\nprocessed_data = processor.process_data()\nassert processed_data == ['A', 'B', 'C']", "gpt-4.1-mini": "        return item.isalpha()"}
{"id": "43", "testsource": "synthbench-low-context", "language": "python", "prefix": "class Animal:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        raise NotImplementedError(\"Subclasses must implement this method\")\n\nclass Dog(Animal):\n    def speak(self):\n        return \"Woof!\"\n\nclass Cat(Animal):\n    def speak(self):\n        return \"Meow!\"\n\n# Adding a new subclass\nclass Parrot(Animal):\n", "suffix": "\n\ndef test_animal_sounds():\n    animals = [Dog('Buddy'), Cat('Whiskers'), Parrot('Polly')]\n    sounds = [animal.speak() for animal in animals]\n    assert sounds == [\"Woof!\", \"Meow!\", \"Squawk!\"]\n\n# Run the test\ntest_animal_sounds()\n", "golden_completion": "    def speak(self):\n        return \"Squawk!\"", "LLM_justification": "This is a good test case because it evaluates the model's ability to recognize and complete the pattern of subclassing and method overriding in object-oriented programming. The prompt provides enough context by showing two subclasses (Dog and Cat) that override the speak method, establishing a clear pattern. The suffix includes a test function that verifies the correct behavior of the added subclass (Parrot). Multiple valid completions are possible, but they must adhere to the established pattern of overriding the speak method to provide a unique sound for each animal.", "assertions": "assert Parrot('Polly').speak() == 'Squawk!'\ntest_animal_sounds()\n", "gpt-4.1-mini": "    def speak(self):\n        return \"Squawk!\""}
{"id": "44", "testsource": "synthbench-low-context", "language": "python", "prefix": "class MyClass:\n    def __init__(self, value):\n        self.value = value\n\n    def increment(self):\n", "suffix": "\n\n# Test the MyClass implementation\nobj = MyClass(10)\nobj.increment()\nassert obj.value == 11\nobj.increment()\nassert obj.value == 12", "golden_completion": "        self.value += 1", "LLM_justification": "This test case is designed to evaluate the model's ability to recognize and complete a common object-oriented pattern: defining methods within a class. The prompt provides the class definition and method signature, with minimal context suggesting that the increment method should modify the instance's value attribute. Multiple valid completions could include various ways of incrementing the value, such as self.value += 1 or self.value = self.value + 1. The provided assertions test the correctness of the increment method and validate the pattern.", "assertions": "assert 'MyClass' in globals()\nassert callable(MyClass(0).increment)\nobj = MyClass(5)\nobj.increment()\nassert obj.value == 6\nobj.increment()\nassert obj.value == 7", "gpt-4.1-mini": "        self.value += 1"}
{"id": "45", "testsource": "synthbench-low-context", "language": "python", "prefix": "class Calculator:\n    def add(self, a, b):\n        return a + b\n\n    def subtract(self, a, b):\n        return a - b\n\n    def multiply(self, a, b):\n        return a * b\n\n    def divide(self, a, b):\n        # Complete the divide method\n", "suffix": "\n# Test cases\ncalc = Calculator()\nassert calc.add(2, 3) == 5\nassert calc.subtract(5, 3) == 2\nassert calc.multiply(2, 3) == 6\nassert calc.divide(6, 3) == 2\nassert calc.divide(5, 2) == 2.5\ntry:\n    calc.divide(5, 0)\nexcept ZeroDivisionError:\n    pass\n", "golden_completion": "        if b == 0:\n            raise ZeroDivisionError('division by zero')\n        return a / b", "LLM_justification": "This is a good test case because it requires the model to recognize the pattern of basic arithmetic operations and correctly implement the division method, including proper error handling for division by zero. The prompt provides sufficient context for recognizing the pattern, and the suffix includes comprehensive assertions to verify the implementation.", "assertions": "assert calc.divide(6, 3) == 2\nassert calc.divide(5, 2) == 2.5\ntry:\n    calc.divide(5, 0)\nexcept ZeroDivisionError:\n    pass", "gpt-4.1-mini": "        if b == 0:\n            raise ZeroDivisionError(\"division by zero\")\n        return a / b"}
{"id": "46", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\nimport time\n\n# A decorator for measuring the execution time of functions\n\ndef timing_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        print(f\"{func.__name__} took {end_time - start_time:.4f} seconds\")\n        return result\n    return wrapper\n\n# Example function to be decorated\n@timing_decorator\ndef example_function(x, y):\n    time.sleep(0.1)\n    return x + y\n\n# Another function to be decorated\n@timing_decorator\ndef another_function(a, b):\n    time.sleep(0.2)\n    return a * b\n\n# Test the decorated functions\nresult1 = example_function(5, 3)\nresult2 = another_function(2, 4)\n\n# Assertions to check the correctness of the decorated functions\n", "suffix": "assert result1 == 8\nassert result2 == 8\n\n# Clean up resources (if any)\n# In this example, there are no resources to clean up.\n", "golden_completion": "assert example_function.__name__ == 'example_function'\nassert another_function.__name__ == 'another_function'\n", "LLM_justification": "This is a good test case because it establishes a common decorator pattern (timing_decorator) and applies it to multiple functions. The pattern is clear and recognizable with minimal context. The assertions check the correctness of the decorated functions and also ensure that the original function names are preserved, demonstrating proper use of functools.wraps in the decorator. Multiple valid completions are possible, as long as the checks for function names and results are included.", "assertions": "assert result1 == 8\nassert result2 == 8\n", "gpt-4.1-mini": "assert result1 == 8"}
{"id": "47", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\nimport time\n\n# Decorator to measure execution time of a function\n\ndef timing_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.perf_counter()\n        result = func(*args, **kwargs)\n        end_time = time.perf_counter()\n        execution_time = end_time - start_time\n        print(f\"Executed {func.__name__} in {execution_time:.4f} seconds\")\n        return result\n    return wrapper\n\n# Example function to decorate\n@timing_decorator\ndef compute_square_sum(n):\n    \"\"\"Compute the sum of squares of numbers from 1 to n\"\"\"\n    return sum(i * i for i in range(1, n + 1))\n\n@timing_decorator\ndef compute_cube_sum(n):\n    \"\"\"Compute the sum of cubes of numbers from 1 to n\"\"\"\n    return sum(i * i * i for i in range(1, n + 1))\n\n@timing_decorator\ndef compute_sum(n):\n    \"\"\"Compute the sum of numbers from 1 to n\"\"\"\n", "suffix": "# Test cases\nresult1 = compute_square_sum(10)\nresult2 = compute_cube_sum(10)\nresult3 = compute_sum(10)\n\nassert result1 == 385, f\"Expected 385 but got {result1}\"\nassert result2 == 3025, f\"Expected 3025 but got {result2}\"\nassert result3 == 55, f\"Expected 55 but got {result3}\"\n", "golden_completion": "    return sum(i for i in range(1, n + 1))", "LLM_justification": "This test case provides a clear example of a decorator pattern in Python, specifically a timing decorator. The prefix establishes the pattern of using the timing_decorator to measure execution time of functions. The missing completion is the body of the compute_sum function, which should follow the same pattern as compute_square_sum and compute_cube_sum. The golden_completion demonstrates understanding of the pattern with minimal context, ensuring the function is correctly implemented. The assertions verify correctness of the decorated functions.", "assertions": "assert compute_square_sum(10) == 385\nassert compute_cube_sum(10) == 3025\nassert compute_sum(10) == 55\n", "gpt-4.1-mini": "    return sum(range(1, n + 1))"}
{"id": "48", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\nfrom collections import namedtuple\n\nclass Event:\n    def __init__(self):\n        self._callbacks = []\n\n    def register(self, callback):\n        self._callbacks.append(callback)\n\n    def unregister(self, callback):\n        self._callbacks.remove(callback)\n\n    def fire(self, *args, **kwargs):\n        for callback in self._callbacks:\n            callback(*args, **kwargs)\n\nclass Button:\n    def __init__(self):\n        self.on_click = Event()\n\nbutton = Button()\n\nEventData = namedtuple('EventData', ['type', 'button'])\n\n@functools.lru_cache()\ndef handle_click(event):\n    print(f'Button clicked with event: {event}')\n\n# Register the callback\nbutton.on_click.register(handle_click)\n\n# Create event data as a namedtuple\nevent_data = EventData(type='click', button='left')\n", "suffix": "button.on_click.fire(event_data)\n", "golden_completion": "handle_click(event_data)\n", "LLM_justification": "This example tests the LLM's ability to complete an event handling pattern. The prefix sets up a button click event system with a registered callback function. The golden completion requires recognizing the pattern of firing an event to trigger the callback, which should print a message. The suffix then tests unregistering the callback and ensuring that firing the event no longer triggers the callback, demonstrating proper event handling and resource cleanup.", "assertions": "assert handle_click.cache_info().hits == 1, 'Callback should have been called once'\nbutton.on_click.unregister(handle_click)\nbutton.on_click.fire(event_data)\nassert handle_click.cache_info().hits == 1, 'Callback should not be called after unregistration'", "gpt-4.1-mini": "button.on_click.fire(event_data)"}
{"id": "49", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\n\ndef logger(func):\n    \"\"\"A decorator that logs the function call details - arguments and return value.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        args_repr = [repr(a) for a in args]                      # 1\n        kwargs_repr = [f\"{k}={v!r}\" for k, v in kwargs.items()]  # 2\n        signature = \", \".join(args_repr + kwargs_repr)          # 3\n        print(f\"Calling {func.__name__}({signature})\")\n        value = func(*args, **kwargs)\n        print(f\"{func.__name__} returned {value!r}\")            # 4\n        return value\n    return wrapper\n\n@logger\ndef say_hello(name):\n    return f\"Hello, {name}!\"\n\n@logger\n", "suffix": "# Testing the decorated functions\nassert say_hello(\"Alice\") == \"Hello, Alice!\"\nassert calculate_sum(3, 4) == 7\n", "golden_completion": "def calculate_sum(a, b):\n    return a + b\n", "LLM_justification": "This test case evaluates the ability of an LLM to recognize and complete a common decorator pattern in Python. The prefix establishes a logging decorator and shows its application on a sample function, `say_hello`. The completion should apply the same decorator to another function `calculate_sum` following the same pattern. The suffix contains tests verifying the correctness of both decorated functions, making it a good test for low-context pattern recognition.", "assertions": "assert say_hello(\"Alice\") == \"Hello, Alice!\"\nassert calculate_sum(3, 4) == 7\n", "gpt-4.1-mini": "def calculate_sum(a, b):\n    return a + b"}
{"id": "50", "testsource": "synthbench-low-context", "language": "python", "prefix": "import functools\nimport time\n\n# Decorator pattern example with timing functionality\nclass TimedCache:\n    def __init__(self, timeout=60):\n        self.cache = {}\n        self.timeout = timeout\n\n    def __call__(self, func):\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            key = (args, tuple(sorted(kwargs.items())))\n            if key in self.cache:\n                result, timestamp = self.cache[key]\n                if time.time() - timestamp < self.timeout:\n                    return result\n            result = func(*args, **kwargs)\n            self.cache[key] = (result, time.time())\n            return result\n        return wrapped\n\ndef slow_function(x, y):\n    time.sleep(2)\n    return x + y\n\n# Applying the decorator\n", "suffix": "\n# Testing the cached function\nstart_time = time.time()\nresult1 = cached_slow_function(2, 3)\nelapsed1 = time.time() - start_time\n\nstart_time = time.time()\nresult2 = cached_slow_function(2, 3)\nelapsed2 = time.time() - start_time\n\nassert result1 == 5\nassert result2 == 5\nassert elapsed1 >= 2\nassert elapsed2 < 2\n\nprint(\"All assertions passed.\")", "golden_completion": "cached_slow_function = TimedCache(timeout=5)(slow_function)\n", "LLM_justification": "This test case checks the LLM's ability to recognize and complete the decorator pattern in Python. The prefix sets up a caching decorator and applies it to a slow function. The golden completion involves reapplying the decorator to the function, which is a common pattern. The suffix includes test code to verify the caching functionality, ensuring that the decorator works as intended and that the function result is cached correctly. This scenario tests the model's understanding of decorators, function wrapping, and caching mechanisms, which are key aspects of Python metaprogramming.", "assertions": "assert result1 == 5\nassert result2 == 5\nassert elapsed1 >= 2\nassert elapsed2 < 2\n", "gpt-4.1-mini": "cached_slow_function = TimedCache(timeout=60)(slow_function)"}
